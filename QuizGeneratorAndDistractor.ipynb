{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766612d2-7bb3-4c70-bf16-83f1adcc99a8",
   "metadata": {},
   "source": [
    "## Pipeline Quiz Generator (Separate Quiz and Distractor Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413f9cf-5025-423f-bf1b-4eb3fa900697",
   "metadata": {},
   "source": [
    "Description: Quiz Generator with separate pipeline for quiz generation and then distractor generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbacf58-f521-404d-af8d-8092a24a441f",
   "metadata": {},
   "source": [
    "### Step 1 : SciQ Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19678d2d-152b-4f66-ac5f-b2fea2fc0bc0",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f69ea9-2967-40f6-9018-1cf30b35c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq_dataset = load_dataset(\"allenai/sciq\")\n",
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03e40d-09c8-4479-8f7a-66af4fa73200",
   "metadata": {},
   "source": [
    "Sample Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1741ece6-e8cb-4a10-9ad5-f1d2c54cac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A small scale version of what type of map displays individual rock units?',\n",
       " 'distractor3': 'polar map',\n",
       " 'distractor1': 'seismic map',\n",
       " 'distractor2': 'geographic map',\n",
       " 'correct_answer': 'geologic map',\n",
       " 'support': 'Geologic maps display rock units and geologic features. A small scale map displays individual rock units while a large scale map shows geologic provinces.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset[\"train\"][27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf6c12-c1e6-4f2f-90f6-230ac55feb72",
   "metadata": {},
   "source": [
    "Drop every data with empty support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37af7ae3-1aa7-478a-be68-4176effd5198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sciq = sciq_dataset.filter(lambda example: example[\"support\"] != '')\n",
    "filtered_sciq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db998f-1e85-47ad-a80c-91a06397562a",
   "metadata": {},
   "source": [
    "Check for support with longer than 512 tokens/words (Maximum token of T5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aab855a-8a1b-4cf9-bc0f-d8a5a9e4e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 165\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = filtered_sciq.filter(lambda example: len(example[\"support\"]) > 2000) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769ade74-e3fa-47ee-902d-e5a5f9dcea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'The level of carbon dioxide in the atmosphere is greatly influenced by the reservoir of carbon where?',\n",
       " 'distractor3': 'after the oceans',\n",
       " 'distractor1': 'before the oceans',\n",
       " 'distractor2': 'in the earth',\n",
       " 'correct_answer': 'in the oceans',\n",
       " 'support': 'As stated, the atmosphere is a major reservoir of carbon in the form of carbon dioxide that is essential to the process of photosynthesis. The level of carbon dioxide in the atmosphere is greatly influenced by the reservoir of carbon in the oceans. The exchange of carbon between the atmosphere and water reservoirs influences how much carbon is found in each, and each one affects the other reciprocally. Carbon dioxide (CO2) from the atmosphere dissolves in water and, unlike oxygen and nitrogen gas, reacts with water molecules to form ionic compounds. Some of these ions combine with calcium ions in the seawater to form calcium carbonate (CaCO3), a major component of the shells of marine organisms. These organisms eventually form sediments on the ocean floor. Over geologic time, the calcium carbonate forms limestone, which comprises the largest carbon reservoir on Earth. On land, carbon is stored in soil as organic carbon as a result of the decomposition of living organisms or from weathering of terrestrial rock and minerals. Deeper under the ground, at land and at sea, are fossil fuels, the anaerobically decomposed remains of plants that take millions of years to form. Fossil fuels are considered a non-renewable resource because their use far exceeds their rate of formation. A non-renewable resource is either regenerated very slowly or not at all. Another way for carbon to enter the atmosphere is from land (including land beneath the surface of the ocean) by the eruption of volcanoes and other geothermal systems. Carbon sediments from the ocean floor are taken deep within Earth by the process of subduction: the movement of one tectonic plate beneath another. Carbon is released as carbon dioxide when a volcano erupts or from volcanic hydrothermal vents. Carbon dioxide is also added to the atmosphere by the animal husbandry practices of humans. The large number of land animals raised to feed Earth’s growing human population results in increased carbon-dioxide levels in the atmosphere caused by their respiration. This is another example of how human activity indirectly affects biogeochemical cycles in a significant way. Although much of the debate about the future effects of increasing atmospheric carbon on climate change focuses on fossils fuels, scientists take natural processes, such as volcanoes, plant growth, soil carbon levels, and respiration, into account as they model and predict the future impact of this increase.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['train'][150]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e924e3-6f09-44b2-8b96-857c53343fe2",
   "metadata": {},
   "source": [
    "We can see above that support is long but only a few sentences is relevant, we cannot do raw summarization, we have to extract text based on keywords which are answers (distractors and keywords from questions too!). If we left this, support and answer will be truncated. If we summarize it raw, we lose important info of what is asked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd5cb9-928f-4a20-b7b4-fc5b56ff8d98",
   "metadata": {},
   "source": [
    "Extractive Summarization based on answer and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c795ce16-5af8-42dd-a38c-68d2e76b7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_question(question):\n",
    "    kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(question)\n",
    "    return [keyword for keyword, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d14d86-d15a-4bb3-8286-a6e63403c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok for tok in doc]\n",
    "    lemmas = [tok.lemma_ for tok in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0fe6f8-2d14-4b5b-a59c-e09ee3a989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence, words):\n",
    "    score = 0\n",
    "    clean_sentences = clean_text(sentence.lower())\n",
    "    for word in words:\n",
    "        if clean_text(word.lower()) in clean_sentences:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f88fa3c-f069-46d1-8311-4987a46119b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Carbon dioxide (CO2) from the atmosphere dissolves in water and, unlike oxygen and nitrogen gas, reacts with water molecules to form ionic compounds. The level of carbon dioxide in the atmosphere is greatly influenced by the reservoir of carbon in the oceans.As stated, the atmosphere is a major reservoir of carbon in the form of carbon dioxide that is essential to the process of photosynthesis'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_support(example, max_length=256):\n",
    "    text = example[\"support\"]\n",
    "    words = extract_question(example[\"question\"])\n",
    "    words.extend([test_data[\"correct_answer\"]])\n",
    "\n",
    "    scored_sentences = (\n",
    "        (i, sentence, score_sentence(sentence, words))\n",
    "        for i, sentence in enumerate(text.split(\".\"))\n",
    "        if any(clean_text(w.lower()) in clean_text(sentence.lower()) for w in words)\n",
    "    )\n",
    "    ranked_sentences = sorted(scored_sentences, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    sentence_in_summary = []\n",
    "    sum_of_sentence = 0\n",
    "    for order, sentence, _ in ranked_sentences:\n",
    "        if sum_of_sentence <= max_length:\n",
    "            sentence_in_summary.append((order, sentence))\n",
    "            sum_of_sentence += len(sentence)\n",
    "\n",
    "    summary = sorted(sentence_in_summary, key=lambda x: x[1])\n",
    "    return \".\".join(sent for _, sent in summary)\n",
    "\n",
    "\n",
    "summarize_support(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0684631-ea06-4391-bb96-e58d874d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(example, sent_size=256):\n",
    "    sentences = \"{}<sep>{}\".format(example['correct_answer'], example['support'])\n",
    "    max_len = sent_size - len(\"{}<sep>\".format(example['correct_answer']))\n",
    "    context = example['support'] if len(sentences) < sent_size else summarize_support(example, max_length=max_len)\n",
    "    \n",
    "    return {\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "preprocessed_sciq = filtered_sciq.map(generate_context, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1823146-ef39-41fe-a6a0-8b51bfc115f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x = preprocessed_sciq.filter(lambda example: len(example[\"support\"]) > 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0c871b-4d92-4235-8ddb-d5e59737bfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?',\n",
       " 'distractor3': 'endothermic',\n",
       " 'distractor1': 'unbalanced',\n",
       " 'distractor2': 'reactive',\n",
       " 'correct_answer': 'exothermic',\n",
       " 'support': 'Summary Changes of state are examples of phase changes, or phase transitions. All phase changes are accompanied by changes in the energy of a system. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. The conversion of a solid to a liquid is called fusion (or melting). The energy required to melt 1 mol of a substance is its enthalpy of fusion (ΔHfus). The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization (ΔHvap). The direct conversion of a solid to a gas is sublimation. The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation (ΔHsub) and is the sum of the enthalpies of fusion and vaporization. Plots of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves. Heating curves relate temperature changes to phase transitions. A superheated liquid, a liquid at a temperature and pressure at which it should be a gas, is not stable. A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature. Instead, they form a supercooled liquid, a metastable liquid phase that exists below the normal melting point. Supercooled liquids usually crystallize on standing, or adding a seed crystal of the same or another substance can induce crystallization.',\n",
       " 'context': ' Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. The conversion of a solid to a liquid is called fusion (or melting)'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd2a16a-80be-47c6-807a-1eeb1f2bac8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f72ad5f9ca4d429e5b209b2453b101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a74f2a0be1413289ede03dbcecea0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dc8a0bc3624fde9b92a0aee0a7910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_sciq.save_to_disk(\"preprocessed_sciq-qg-256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4e16d-63b7-4ce8-980d-d4b5a8c04efe",
   "metadata": {},
   "source": [
    "Now preprocessed has shorter context for long ones, due to map process running slow, uncomment below after getting zip file from me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5bf3694-b698-47c6-a91d-6aea4443544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# preprocessed_sciq = load_from_disk(\"preprocessed_sciq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3020c7f-3b2e-4634-b55b-62dde2253b1d",
   "metadata": {},
   "source": [
    "### Step 2 Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c47179-e1f2-48f5-8e72-2de0111700d8",
   "metadata": {},
   "source": [
    "#### Tokenize for Question Generation \n",
    "\n",
    "Input : Context and Answer \\\n",
    "Output : Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd6ea20-1618-49b8-a276-a5bc697ffefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c730acd-e5d0-4544-9caa-b4113c3838ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# question = 'The generation of an isolated but open system, which we might call a protocell, was a critical step in the origin of life. Such an isolated system has important properties that are likely to have facilitated the further development of life. For example, because of the membrane boundary, changes that occur within one such structure will not be shared with neighboring systems. Rather, they accumulated in, and favor the survival of, one system over its neighbors. Such systems can also reproduce in a crude way by fragmentation. If changes within one such system improved its stability, its ability to accumulate resources, or its ability to survive and reproduce, that system, and its progeny, would be likely to become more common. As these changes accumulate and are passed from parent to offspring, the organisms will inevitably evolve, as we will see in detail in the next chapter. As in living systems today, the earliest steps in the formation of the first organisms required a source of energy to maintain the non-equilibrium living system. There are really two choices for the source of this energy, either light (electromagnetic radiation from the sun) or thermodynamically unstable chemicals present in the environment. There have been a number of plausible scenarios, based on various observations, for the steps leading to life. For example, a recent study based on the analysis of the genes (and the proteins that they encode) found in modern organisms, suggests that the last universal common ancestor (LUCA) arose in association with hydrothermal vents.60 But whether this reflects LUCA or an ancestor of LUCA that became adapted to living is association with hydrothermal vents is difficult (and perhaps impossible) to resolve unambiguously, particularly since LUCA lived ~3.4-3.8 billion years ago and cannot be studied directly. Mapping the history of life on earth Assuming, as seems likely, that life arose spontaneously, we can now look at what we know about the fossil record to better understand the diversification of life and life’s impact on the Earth. This is probably best done by starting with what we know about where the Universe and Earth came from. The current scientific model for the origin of the universe is known as the “Big Bang” (also known as the “primeval atom” or the “cosmic egg”), an idea originally proposed by the priest, physicist and astronomer Georges Lemaître (1894-1966).61 The Big Bang model arose from efforts to answer the question of whether the fuzzy nebulae identified by astronomers were located within or outside of our galaxy. This required some way to determine how far these nebulae were from Earth. Edwin Hubble (1889-1953) and his co-workers were the first to realize that nebulae were in fact galaxies in their own right, each very much like our own Milky Way and each is composed of many billions of stars. This was a surprising result. It made Earth, sitting on the edge of one (the Milky Way) among many, many galaxies seem less important – a change in cosmological perspective similar to that associated with the idea that the Sun, rather than Earth, was the center of the solar system (and the Universe). To measure the movement of galaxies with respect to Earth, Hubble and colleagues combined to types of observations. The first of these allowed them to estimate the distance from the Earth to.'\n",
    "# tokenized_targets = tokenizer.encode_plus(question, max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "# tokenized_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90b92480-d5d3-40e8-bc35-6a4a198ab854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in tokenized_targets['input_ids'][:2]:\n",
    "#     print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3316e356-7e22-4385-9dba-e395bb0b890e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(example):\n",
    "    text = \"{}<sep>{}\".format(example['correct_answer'], example['context'])\n",
    "    question = example['question']\n",
    "\n",
    "    max_length = 256\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(question, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset = preprocessed_sciq.map(preprocess_dataset, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79cb7c31-3517-464d-a4bc-d1d7cb407950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"bleu\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9578193a-0133-4cc9-88e1-a770bc95e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pretrained_question_generation\", \n",
    "    evaluation_strategy=\"no\", \n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    "    # gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924faebf-8d6c-4c34-a225-b3d91192bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdc93e-fb24-4690-92df-b4834b850940",
   "metadata": {},
   "source": [
    "There is no validation yet (it's buggy in my comp for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d590270d-838c-4883-a60a-432c5ee502ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1965' max='1965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1965/1965 1:41:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.215800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory pretrained_question_generation/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory pretrained_question_generation/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory pretrained_question_generation/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1965, training_loss=1.3112742339078403, metrics={'train_runtime': 6070.7844, 'train_samples_per_second': 5.179, 'train_steps_per_second': 0.324, 'total_flos': 2126625726529536.0, 'train_loss': 1.3112742339078403, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6207499c-fd38-4fa8-af43-3c5105e1dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model-qg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0448974-f855-4242-b0ff-c24f0cb01ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [11499,\n",
       "  32100,\n",
       "  37,\n",
       "  2677,\n",
       "  5013,\n",
       "  2107,\n",
       "  387,\n",
       "  45,\n",
       "  3,\n",
       "  9,\n",
       "  6957,\n",
       "  42,\n",
       "  13064,\n",
       "  616,\n",
       "  190,\n",
       "  6079,\n",
       "  9243,\n",
       "  7293,\n",
       "  7,\n",
       "  6,\n",
       "  114,\n",
       "  273,\n",
       "  16,\n",
       "  7996,\n",
       "  666,\n",
       "  3,\n",
       "  5,\n",
       "  100,\n",
       "  19,\n",
       "  2953,\n",
       "  57,\n",
       "  579,\n",
       "  2677,\n",
       "  11,\n",
       "  23295,\n",
       "  24,\n",
       "  169,\n",
       "  8,\n",
       "  387,\n",
       "  12,\n",
       "  1633,\n",
       "  70,\n",
       "  4096,\n",
       "  5,\n",
       "  634,\n",
       "  52,\n",
       "  1982,\n",
       "  10441,\n",
       "  19,\n",
       "  10441,\n",
       "  24,\n",
       "  3033,\n",
       "  7,\n",
       "  8,\n",
       "  2912,\n",
       "  13,\n",
       "  387,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [366,\n",
       "  8,\n",
       "  2912,\n",
       "  13,\n",
       "  387,\n",
       "  19,\n",
       "  1936,\n",
       "  227,\n",
       "  271,\n",
       "  261,\n",
       "  16,\n",
       "  9243,\n",
       "  6,\n",
       "  34,\n",
       "  19,\n",
       "  48,\n",
       "  607,\n",
       "  13,\n",
       "  10441,\n",
       "  58,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0878dabd-96e9-4dad-bc00-f8ed656e6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"model-qg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "72a5ee4c-cc30-47da-8a4e-d19c2c487335",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"{}<sep>{}\".format('minute amounts', \"Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap.\")\n",
    "tokenized_inputs = tokenizer.encode_plus(text, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a76f73ad-4d26-498f-9d65-1eaa0c961472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  363,   19,    8, 1657,   21,    3,    9, 6567,   29,    6, 6567,\n",
       "           29,   15,    6,   42, 9077, 2358,   24, 1472]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids=tokenized_inputs['input_ids']\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b5489e6c-e454-4a85-86ae-927611171354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>What is the term for a neuron, neurone, or nerve cell that fire\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ab1deaa-239a-427e-90f3-5def87964780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When the temperature of water is increased after being used in cooling, it is this form of pollution?',\n",
       " 'distractor3': 'air',\n",
       " 'distractor1': 'atmospheric',\n",
       " 'distractor2': 'cosmic',\n",
       " 'correct_answer': 'thermal',\n",
       " 'support': \"Thermal pollution is pollution that raises the temperature of water. This is caused by power plants and factories that use the water to cool their machines. The plants pump cold water from a lake or coastal area through giant cooling towers, like those in Figure below . As it flows through the towers, the cold water absorbs heat. This warmed water is returned to the lake or sea. Thermal pollution can kill fish and other water life. It's not just the warm temperature that kills them. Warm water can’t hold as much oxygen as cool water. If the water gets too warm, there may not be enough oxygen for living things.\",\n",
       " 'context': ' The plants pump cold water from a lake or coastal area through giant cooling towers, like those in Figure below . This is caused by power plants and factories that use the water to cool their machines.Thermal pollution is pollution that raises the temperature of water'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sciq['validation'][137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "762c05e2-9a7c-4366-93ae-e346a5924dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in tokenized_dataset['validation'][137]['input_ids']:\n",
    "#     print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6c7a8ca-1546-4762-b75b-da4c84480dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/887 04:17 < 23:05, 0.54 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.32 GiB (GPU 0; 6.00 GiB total capacity; 5.98 GiB already allocated; 0 bytes free; 8.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   3230\u001b[0m     eval_dataloader,\n\u001b[1;32m   3231\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3232\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   3233\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   3234\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3235\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   3236\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   3237\u001b[0m )\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3444\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3442\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3443\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m-> 3444\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(preds_host, logits, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m   3446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3447\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:125\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    128\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:84\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     81\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((tensor1, tensor2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     87\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.32 GiB (GPU 0; 6.00 GiB total capacity; 5.98 GiB already allocated; 0 bytes free; 8.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4d657-fd13-47c3-be87-a2633626c43e",
   "metadata": {},
   "source": [
    "### Step 3 Distractor Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f36d75-49b1-45ef-b1f9-a8db16698a92",
   "metadata": {},
   "source": [
    "#### Tokenize for Distractor Generation \n",
    "\n",
    "Input : Answer, Question, Context \\\n",
    "Output : 3 Distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2ed2d55-ff0e-417b-825f-833f8ab35c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2148cf41ff894863b06440cb46e336f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afaa3159db7649af90e26640d93292fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32784d6a1c3d4b29899249cddeedb73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset_for_distractor(example):\n",
    "    text = \"{}<sep>{}<sep>{}\".format(example['question'], example['correct_answer'], example['context'])\n",
    "    distractor = \"{}<sep>{}<sep>{}\".format(example['distractor1'], example['distractor2'], example['distractor3'])\n",
    "\n",
    "    max_length = 256\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(distractor, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset_distractor = preprocessed_sciq.map(preprocess_dataset_for_distractor, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "73d38267-cb3e-4309-bcfd-0c1c2f411e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_dis = TrainingArguments(\n",
    "    output_dir=\"pretrained_distractor_generation\", \n",
    "    evaluation_strategy=\"no\", \n",
    "    logging_strategy=\"epoch\",\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    "    # gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "model_dis = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_dis = model_dis.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_dis,\n",
    "    args=training_args_dis,\n",
    "    train_dataset=tokenized_dataset_distractor[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_distractor[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d812e71-345e-4343-9fac-6eac2bf4204e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1965' max='1965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1965/1965 1:45:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>1.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1311</td>\n",
       "      <td>1.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>1.661900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1965, training_loss=1.7532189910345102, metrics={'train_runtime': 6312.5134, 'train_samples_per_second': 4.981, 'train_steps_per_second': 0.311, 'total_flos': 2126625726529536.0, 'train_loss': 1.7532189910345102, 'epoch': 3.0})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "27346c6f-03ac-439d-ba03-bc171864e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model-dg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1af6cf19-5479-4f9d-b008-c0729667025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dis = T5ForConditionalGeneration.from_pretrained(\"model-dg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d4f35c3-b3cb-4546-b1d5-88e29f1492bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Compound forms when atoms of nonmetals form molecules that are held together by what?',\n",
       " 'distractor3': 'dissonance bonds',\n",
       " 'distractor1': 'phenotype bonds',\n",
       " 'distractor2': 'magnetic bonds',\n",
       " 'correct_answer': 'covalent bonds',\n",
       " 'support': 'Compound forms when atoms of nonmetals form molecules that are held together by covalent bonds.',\n",
       " 'context': 'Compound forms when atoms of nonmetals form molecules that are held together by covalent bonds.'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = preprocessed_sciq['test'][137]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1bd4002e-e0d0-4268-a416-8cfcc1b7144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"{}<sep>{}<sep>{}\".format(test_data['question'], test_data['correct_answer'], test_data['context'])\n",
    "tokenized_inputs = tokenizer.encode_plus(text, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a133623b-110f-4b00-bc73-1af434db989f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   576, 15592, 13237,     2,     7,    15,   102,  3155,   509,\n",
       "         15592, 13237,     2,     7,    15,   102,  3155,   509, 15592, 13237]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_dis.generate(\n",
    "    input_ids=tokenized_inputs['input_ids']\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c762ab87-d6fe-4b0b-b197-831c3f690ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> covalent bonds<unk> sep>covalent bonds<unk>sep>covalent bonds\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
