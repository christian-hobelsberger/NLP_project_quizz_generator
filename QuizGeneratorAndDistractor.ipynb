{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766612d2-7bb3-4c70-bf16-83f1adcc99a8",
   "metadata": {},
   "source": [
    "## Pipeline Quiz Generator (Separate Quiz and Distractor Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413f9cf-5025-423f-bf1b-4eb3fa900697",
   "metadata": {},
   "source": [
    "Description: Quiz Generator with separate pipeline for quiz generation and then distractor generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbacf58-f521-404d-af8d-8092a24a441f",
   "metadata": {},
   "source": [
    "### Step 1 : SciQ Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19678d2d-152b-4f66-ac5f-b2fea2fc0bc0",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f69ea9-2967-40f6-9018-1cf30b35c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq_dataset = load_dataset(\"allenai/sciq\")\n",
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03e40d-09c8-4479-8f7a-66af4fa73200",
   "metadata": {},
   "source": [
    "Sample Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1741ece6-e8cb-4a10-9ad5-f1d2c54cac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A small scale version of what type of map displays individual rock units?',\n",
       " 'distractor3': 'polar map',\n",
       " 'distractor1': 'seismic map',\n",
       " 'distractor2': 'geographic map',\n",
       " 'correct_answer': 'geologic map',\n",
       " 'support': 'Geologic maps display rock units and geologic features. A small scale map displays individual rock units while a large scale map shows geologic provinces.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset[\"train\"][27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf6c12-c1e6-4f2f-90f6-230ac55feb72",
   "metadata": {},
   "source": [
    "Drop every data with empty support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37af7ae3-1aa7-478a-be68-4176effd5198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sciq = sciq_dataset.filter(lambda example: example[\"support\"] != '')\n",
    "filtered_sciq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db998f-1e85-47ad-a80c-91a06397562a",
   "metadata": {},
   "source": [
    "Check for support with longer than 512 tokens/words (Maximum token of T5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aab855a-8a1b-4cf9-bc0f-d8a5a9e4e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 3029\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 257\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 267\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = filtered_sciq.filter(lambda example: len(example[\"support\"]) > 512) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769ade74-e3fa-47ee-902d-e5a5f9dcea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Interstitial carbides are produced by the reaction of most transition metals at high temperatures with what element?',\n",
       " 'distractor3': 'nitrogen',\n",
       " 'distractor1': 'hydrogen',\n",
       " 'distractor2': 'oxygen',\n",
       " 'correct_answer': 'carbon',\n",
       " 'support': 'temperatures with electropositive metals such as those of groups 1 and 2 and aluminum produces ionic carbides, which contain discrete metal cations and carbon anions. The identity of the anions depends on the size of the second element. For example, smaller elements such as beryllium and aluminum give methides such as Be2C and Al4C3, which formally contain the C4− ion derived from methane (CH4) by losing all four H atoms as protons. In contrast, larger metals such as sodium and calcium give carbides with stoichiometries of Na2C2 and CaC2. Because these carbides contain the C4− ion, which is derived from acetylene (HC≡CH) by losing both H atoms as protons, they are more properly called acetylides. As discussed in Chapter 21 \"Periodic Trends and the \", Section 21.4 \"The Alkaline Earth Metals (Group 2)\", reacting ionic carbides with dilute aqueous acid results in protonation of the anions to give the parent hydrocarbons: CH4 or C2H2. For many years, miners’ lamps used the reaction of calcium carbide with water to produce a steady supply of acetylene, which was ignited to provide a portable lantern. The reaction of carbon with most transition metals at high temperatures produces interstitial carbides. Due to the less electropositive nature of the transition metals, these carbides contain covalent metal– carbon interactions, which result in different properties: most interstitial carbides are good conductors of electricity, have high melting points, and are among the hardest substances known. Interstitial carbides exhibit a variety of nominal compositions, and they are often nonstoichiometric compounds whose carbon content can vary over a wide range. Among the most important are tungsten carbide (WC), which is used industrially in high-speed cutting tools, and cementite (Fe3C), which is a major component of steel. Elements with an electronegativity similar to that of carbon form covalent carbides, such as silicon carbide (SiC; Equation 22.15) and boron carbide (B4C). These substances are extremely hard, have high melting points, and are chemically inert. For example, silicon carbide is highly resistant to chemical attack at temperatures as high as 1600°C. Because it also maintains its strength at high temperatures, silicon carbide is used in heating elements for electric furnaces and in variable-temperature resistors.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['train'][9]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e924e3-6f09-44b2-8b96-857c53343fe2",
   "metadata": {},
   "source": [
    "We can see above that support is long but only a few sentences is relevant, we cannot do raw summarization, we have to extract text based on keywords which are answers (distractors and keywords from questions too!). If we left this, support and answer will be truncated. If we summarize it raw, we lose important info of what is asked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd5cb9-928f-4a20-b7b4-fc5b56ff8d98",
   "metadata": {},
   "source": [
    "Extractive Summarization based on answer and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c795ce16-5af8-42dd-a38c-68d2e76b7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_question(question):\n",
    "    kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(question)\n",
    "    return [keyword for keyword, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d14d86-d15a-4bb3-8286-a6e63403c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok for tok in doc]\n",
    "    lemmas = [tok.lemma_ for tok in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0fe6f8-2d14-4b5b-a59c-e09ee3a989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence, words):\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        if clean_text(word.lower()) in clean_text(sentence.lower()):\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f88fa3c-f069-46d1-8311-4987a46119b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reaction of carbon with most transition metals at high temperatures produces interstitial carbides. Due to the less electropositive nature of the transition metals, these carbides contain covalent metal– carbon interactions, which result in different properties: most interstitial carbides are good conductors of electricity, have high melting points, and are among the hardest substances known. Interstitial carbides exhibit a variety of nominal compositions, and they are often nonstoichiometric compounds whose carbon content can vary over a wide range'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_support(example, max_length=500):\n",
    "    text = example['support']\n",
    "    words = extract_question(example['question'])\n",
    "    words.extend([test_data['correct_answer']])\n",
    "    \n",
    "    scored_sentences = ((sentence, score_sentence(sentence, words)) for sentence in text.split(\".\") if any(clean_text(w.lower()) in clean_text(sentence.lower()) for w in words))\n",
    "    ranked_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sentence_in_summary = []\n",
    "    sum_of_sentence = 0\n",
    "    for sentences, _ in ranked_sentences:\n",
    "        if sum_of_sentence <= max_length:\n",
    "            sentence_in_summary.append(sentences)\n",
    "            sum_of_sentence += len(sentences)\n",
    "            \n",
    "    return '.'.join(sentence_in_summary)\n",
    "\n",
    "summarize_support(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0684631-ea06-4391-bb96-e58d874d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(example, token_size=512):\n",
    "    sentences = \"{}<sep>{}\".format(example['correct_answer'], example['support'])\n",
    "    max_len = token_size - len(\"{}<sep>\".format(example['correct_answer']))\n",
    "    context = example['support'] if len(sentences) < token_size else summarize_support(example, max_length=max_len)\n",
    "    \n",
    "    return {\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "# preprocessed_sciq = filtered_sciq.map(generate_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1823146-ef39-41fe-a6a0-8b51bfc115f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_x = preprocessed_sciq.filter(lambda example: len(example[\"support\"]) > 3000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0c871b-4d92-4235-8ddb-d5e59737bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = test_x['train'][9]\n",
    "# test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdd2a16a-80be-47c6-807a-1eeb1f2bac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_sciq.save_to_disk(\"preprocessed_sciq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4e16d-63b7-4ce8-980d-d4b5a8c04efe",
   "metadata": {},
   "source": [
    "Now preprocessed has shorter context for long ones, due to map process running slow, uncomment below after getting zip file from me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5bf3694-b698-47c6-a91d-6aea4443544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "preprocessed_sciq = load_from_disk(\"preprocessed_sciq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3020c7f-3b2e-4634-b55b-62dde2253b1d",
   "metadata": {},
   "source": [
    "### Step 2 Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c47179-e1f2-48f5-8e72-2de0111700d8",
   "metadata": {},
   "source": [
    "#### Tokenize for Question Generation \n",
    "\n",
    "Input : Context and Answer \\\n",
    "Output : Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd6ea20-1618-49b8-a276-a5bc697ffefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3316e356-7e22-4385-9dba-e395bb0b890e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(example):\n",
    "    text = \"{}<sep>{}\".format(example['correct_answer'], example['context'])\n",
    "    question = example['question']\n",
    "\n",
    "    max_length = 512\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(question, max_length=max_length, padding='max_length', return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset = preprocessed_sciq.map(preprocess_dataset, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79cb7c31-3517-464d-a4bc-d1d7cb407950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 10:57:07.631163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 10:57:07.631228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 10:57:07.716932: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 10:57:07.896755: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 10:57:09.182447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9578193a-0133-4cc9-88e1-a770bc95e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pretrained_question_gen\", \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "924faebf-8d6c-4c34-a225-b3d91192bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590270d-838c-4883-a60a-432c5ee502ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5241' max='15720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5241/15720 04:14 < 3:06:17, 0.94 it/s, Epoch 1.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23/444 00:32 < 10:19, 0.68 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint = 'pretrained_question_gen/checkpoint-5000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4d657-fd13-47c3-be87-a2633626c43e",
   "metadata": {},
   "source": [
    "### Step 3 Distractor Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f36d75-49b1-45ef-b1f9-a8db16698a92",
   "metadata": {},
   "source": [
    "#### Tokenize for Distractor Generation \n",
    "\n",
    "Input : Answer, Question, Context \\\n",
    "Output : 3 Distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2ed2d55-ff0e-417b-825f-833f8ab35c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1453bcc62736463da11f5ccb5dbd3e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b482ede92948238c8aeca324af2d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b0d15fa55943df8d91448f46d8da46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset_for_distractor(example):\n",
    "    text = \"{} {} {}\".format(example['question'], example['correct_answer'], example['support'])\n",
    "    distractor = \"{} {} {}\".format(example['distractor1'], example['distractor2'], example['distractor3'])\n",
    "\n",
    "    max_length = 512\n",
    "    doc_stride = 128\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', pad_to_max_length=False, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(distractor, max_length=max_length, padding='max_length', pad_to_max_length=False, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids']\n",
    "    input_attention = tokenized_inputs['attention_mask']\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids']\n",
    "    target_attention = tokenized_targets['attention_mask']\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset_distractor = sciq_dataset.map(preprocess_dataset_for_distractor)\n",
    "tokenized_dataset_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d38267-cb3e-4309-bcfd-0c1c2f411e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=2)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
