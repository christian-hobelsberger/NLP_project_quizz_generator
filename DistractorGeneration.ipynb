{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766612d2-7bb3-4c70-bf16-83f1adcc99a8",
   "metadata": {},
   "source": [
    "## Pipeline Quiz Generator (Separate Quiz and Distractor Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413f9cf-5025-423f-bf1b-4eb3fa900697",
   "metadata": {},
   "source": [
    "Description: Quiz Generator with separate pipeline for quiz generation and then distractor generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbacf58-f521-404d-af8d-8092a24a441f",
   "metadata": {},
   "source": [
    "### Step 1 : SciQ Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19678d2d-152b-4f66-ac5f-b2fea2fc0bc0",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f69ea9-2967-40f6-9018-1cf30b35c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq_dataset = load_dataset(\"allenai/sciq\")\n",
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37af7ae3-1aa7-478a-be68-4176effd5198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10263\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 864\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 867\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sciq = sciq_dataset.filter(lambda example: example[\"support\"] != '')\n",
    "filtered_dataset = sciq_dataset.filter(lambda example: example['support'] is not None and example['support'] != \"\")\n",
    "# And include only questions with no superfluous information\n",
    "filtered_dataset = filtered_dataset.filter(lambda example: len(example['question']) < 171)\n",
    "# And remove any datapoints which contain questions that have a 'fill-in-the-blank' type answer\n",
    "filtered_sciq = filtered_dataset.filter(lambda example: '_______' not in example['question'] and '______' not in example['question'] and '_____' not in example['question'] and '____' not in example['question'] and '___' not in example['question'])\n",
    "filtered_sciq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcdbc4a6-7595-41ff-88c0-189c8d0bc9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>distractor3</th>\n",
       "      <th>distractor1</th>\n",
       "      <th>distractor2</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The work-energy theorem states that the net wo...</td>\n",
       "      <td>residual energy</td>\n",
       "      <td>binary energy</td>\n",
       "      <td>new energy</td>\n",
       "      <td>kinetic energy</td>\n",
       "      <td>How the Work-Energy Theorem Applies Now let us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is considered to be the \"fundamental unit...</td>\n",
       "      <td>bacteria</td>\n",
       "      <td>proton</td>\n",
       "      <td>organ</td>\n",
       "      <td>cell</td>\n",
       "      <td>It could easily be said that a cell is the fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the group that rats are ap...</td>\n",
       "      <td>Mammels</td>\n",
       "      <td>primates</td>\n",
       "      <td>Animals</td>\n",
       "      <td>rodents</td>\n",
       "      <td>Rats are mammals, but this class can be divide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What characteristics of an organism help it su...</td>\n",
       "      <td>Changes</td>\n",
       "      <td>systems</td>\n",
       "      <td>additions</td>\n",
       "      <td>adaptations</td>\n",
       "      <td>Some of the characteristics an organism has ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Screws move objects to a higher elevation by i...</td>\n",
       "      <td>kinetic energy</td>\n",
       "      <td>velocity</td>\n",
       "      <td>torque</td>\n",
       "      <td>force applied</td>\n",
       "      <td>The spiral staircase in Figure below also cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      distractor3  \\\n",
       "0  The work-energy theorem states that the net wo...  residual energy   \n",
       "1  What is considered to be the \"fundamental unit...         bacteria   \n",
       "2  What is the name of the group that rats are ap...          Mammels   \n",
       "3  What characteristics of an organism help it su...          Changes   \n",
       "4  Screws move objects to a higher elevation by i...   kinetic energy   \n",
       "\n",
       "     distractor1 distractor2  correct_answer  \\\n",
       "0  binary energy  new energy  kinetic energy   \n",
       "1         proton       organ            cell   \n",
       "2       primates     Animals         rodents   \n",
       "3        systems   additions     adaptations   \n",
       "4       velocity      torque   force applied   \n",
       "\n",
       "                                             support  \n",
       "0  How the Work-Energy Theorem Applies Now let us...  \n",
       "1  It could easily be said that a cell is the fun...  \n",
       "2  Rats are mammals, but this class can be divide...  \n",
       "3  Some of the characteristics an organism has ma...  \n",
       "4  The spiral staircase in Figure below also cont...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(filtered_sciq[\"train\"].shuffle())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db998f-1e85-47ad-a80c-91a06397562a",
   "metadata": {},
   "source": [
    "Check for support with longer than 512 tokens/words (Maximum token of T5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8ef979-2b27-49ee-83ae-ee66f3ea774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3559\n"
     ]
    }
   ],
   "source": [
    "print(df_train['support'].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aab855a-8a1b-4cf9-bc0f-d8a5a9e4e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 9\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = filtered_sciq.filter(lambda example: len(example[\"support\"]) > 3000) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769ade74-e3fa-47ee-902d-e5a5f9dcea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"would not be surprised if the surviving populations experienced serious bottlenecks. The subsequent diversification of the surviving organisms, such as the Dinosauria (which includes the extinct dinosaurs and modern birds) and the Cynodontia, which includes the ancestors of modern mammals, including us, could be due in part to these bottleneck-associated effects, for example, through the removal of competing species or predators. An astreroid impact, known as the Cretaceous-Tertiary event, occurred ~65 million years ago; it contributed to the extinction of the dinosaurs and led to the diversification of mammals (which had first appeared in the fossil record ~160 million years ago). While surviving an asteroid impact (or other dramatic changes in climate) may be random, in other cases who survives a bottleneck is not. Consider the effects of a severe drought or highly virulent bacterial or viral infection; the organisms that survive may have specific phenotypes (and associated genotypes) that significantly influence their chance of survival. In such a case, the effect of the bottleneck event would produce non-random changes in the distribution of genotypes (and alleles) in the post-bottleneck population – these selective effects could continue to influence the population in various ways. For example, a trait associated with pathogen resistance may also have negative phenotypic effects. After the pathogen-driven bottleneck, mutations that mitigate the resistance trait's negative effects (and may have their own effects) can be selected. The end result is that traits that would not be selected in the absence of the pathogen, are selected. In addition, the very occurrence of a rapid and extreme reduction in population size has its own effects. For example, it would be expected to increase the effects of genetic drift (see below) and could make finding a mate more difficult. We can identify extreme population reduction events, such as founder effects and bottlenecks, by looking at the variation in genotypes (that is, the sequence of DNA molecules), particularly in genotypic changes not expected to influence phenotypes, mating preference, or reproductive success. These so-called neutral polymorphisms are expected to accumulate in the regions of the genome between genes (intragenic regions) at a constant rate over time (can you suggest why?) The rate of the accumulation of neutral polymorphisms serves as a type of population-based biological clock. Its rate can be estimated, at least roughly, by comparing the genotypes of individuals of different populations whose time of separation can be accurately estimated (assuming of course that there has been no migrations between the populations). Such studies of genomic sequence data (which we will return to later in much greater detail) indicate that the human population arose in Africa ~500,000 years ago. 119 Before this, the population leading to humans is thought to have undergone a bottleneck around ~1.2 million years ago.120 Once established, groups of modern humans migrated within and out of African, undergoing a series of founder effect events between ~45,000 to 60,000 years ago first as they migrated from southern Africa into the regions of the Horn of Africa, then into the Arabian peninsula, and from there into Europe, Asia, Oceania, and finally the Americas. (↓) Comparing genotypes, that is, neutral polymorphisms, between 119.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['train'][7]\n",
    "test_data['support']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e924e3-6f09-44b2-8b96-857c53343fe2",
   "metadata": {},
   "source": [
    "We can see above that support is long but only a few sentences is relevant, we cannot do raw summarization, we have to extract text based on keywords which are answers (distractors and keywords from questions too!). If we left this, support and answer will be truncated. If we summarize it raw, we lose important info of what is asked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd5cb9-928f-4a20-b7b4-fc5b56ff8d98",
   "metadata": {},
   "source": [
    "Extractive Summarization based on answer and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c795ce16-5af8-42dd-a38c-68d2e76b7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_question(question):\n",
    "    kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(question)\n",
    "    return [keyword for keyword, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d14d86-d15a-4bb3-8286-a6e63403c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok for tok in doc]\n",
    "    lemmas = [tok.lemma_ for tok in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0fe6f8-2d14-4b5b-a59c-e09ee3a989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence, words):\n",
    "    score = 0\n",
    "    clean_sentences = clean_text(sentence.lower())\n",
    "    for word in words:\n",
    "        if clean_text(word.lower()) in clean_sentences:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f88fa3c-f069-46d1-8311-4987a46119b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (↓) Comparing genotypes, that is, neutral polymorphisms, between 119. Its rate can be estimated, at least roughly, by comparing the genotypes of individuals of different populations whose time of separation can be accurately estimated (assuming of course that there has been no migrations between the populations). These so-called neutral polymorphisms are expected to accumulate in the regions of the genome between genes (intragenic regions) at a constant rate over time (can you suggest why?) The rate of the accumulation of neutral polymorphisms serves as a type of population-based biological clock.120 Once established, groups of modern humans migrated within and out of African, undergoing a series of founder effect events between ~45,000 to 60,000 years ago first as they migrated from southern Africa into the regions of the Horn of Africa, then into the Arabian peninsula, and from there into Europe, Asia, Oceania, and finally the Americas'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_support(example, max_words=256):\n",
    "    text = example[\"support\"]\n",
    "    words = extract_question(example[\"question\"])\n",
    "    words.extend([test_data[\"correct_answer\"]])\n",
    "\n",
    "    scored_sentences = (\n",
    "        (i, sentence, score_sentence(sentence, words))\n",
    "        for i, sentence in enumerate(text.split(\".\"))\n",
    "        if any(clean_text(w.lower()) in clean_text(sentence.lower()) for w in words)\n",
    "    )\n",
    "    ranked_sentences = sorted(scored_sentences, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    sentence_in_summary = []\n",
    "    sum_of_words = 0\n",
    "    for order, sentence, _ in ranked_sentences:\n",
    "        num_of_words = len(sentence.split())\n",
    "        if sum_of_words + num_of_words < max_words:\n",
    "            sentence_in_summary.append((order, sentence))\n",
    "            sum_of_words += num_of_words \n",
    "\n",
    "    summary = sorted(sentence_in_summary, key=lambda x: x[1])\n",
    "    return \".\".join(sent for _, sent in summary)\n",
    "\n",
    "summarize_support(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0684631-ea06-4391-bb96-e58d874d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_with_question(example, max_token_size=512):\n",
    "    answer_size = len(example['correct_answer'].split())\n",
    "    question_size = len(example['question'].split())\n",
    "    support_size = len(example['support'].split())\n",
    "\n",
    "    # Add into account separator (<sep>) which are 2\n",
    "    words_len = answer_size + support_size + question_size + 2\n",
    "    context = example['support']\n",
    "\n",
    "    if words_len > max_token_size:\n",
    "        max_new_token_size = max_token_size - answer_size - question_size - 2\n",
    "        context = summarize_support(example, max_words=max_new_token_size)\n",
    "    \n",
    "    return {\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "preprocessed_sciq = filtered_sciq.map(generate_context_with_question, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd2a16a-80be-47c6-807a-1eeb1f2bac8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46df88665b54b4e9dce449e2a2d09ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ad56b8f21647f7804c022f1b2dc991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b3ef38a6b84078959f3ffd1968a5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_sciq.save_to_disk(\"preprocessed_sciq-dg-512-new-RUTGER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4e16d-63b7-4ce8-980d-d4b5a8c04efe",
   "metadata": {},
   "source": [
    "Now preprocessed has shorter context for long ones, due to map process running slow, uncomment below after getting zip file from me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5bf3694-b698-47c6-a91d-6aea4443544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# preprocessed_sciq = load_from_disk(\"preprocessed_sciq-dg-512-new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3020c7f-3b2e-4634-b55b-62dde2253b1d",
   "metadata": {},
   "source": [
    "### Step 2 Distractor Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c47179-e1f2-48f5-8e72-2de0111700d8",
   "metadata": {},
   "source": [
    "#### Tokenize for Distractor Generation\n",
    "\n",
    "Input : Context and Answer and Question\n",
    "Output : 3 Distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd6ea20-1618-49b8-a276-a5bc697ffefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6ba48db-1f2e-4d64-91e9-34a693406f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60071fb7f1de442fb40974e961e143a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4ff791afc343948aac933ee6aebb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873c459a57574fd5af5b33b4721f275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10263\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 864\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 867\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(example):\n",
    "    text = \"Generate distractors for question: {}, answer: {}, context: {}\".format(example['question'], example['correct_answer'], example['context'])\n",
    "    distractor = \"Distractors: {}, {}, {}\".format(example['distractor1'], example['distractor2'], example['distractor3'])\n",
    "\n",
    "    max_length = 512\n",
    "    max_length_target=32\n",
    "    \n",
    "    tokenized_inputs = tokenizer(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer(distractor, max_length=max_length_target, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': target_ids \n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset = preprocessed_sciq.map(preprocess_dataset, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ffecbea-b59b-4a79-824b-294bf9bd7d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d1f5059-57ab-4fed-9b00-e7c2b089f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=2)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5278003-a089-4423-bade-02825fe217d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=3, min_delta=0.5):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11830c95-ac26-43e4-8315-cf2851bd4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1236c02-2147-449c-8443-9685befc493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def validate(model, eval_dataloader):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    model.eval()\n",
    "    progress_bar_val = tqdm(range(len(eval_dataloader)), leave=False)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        active_loss = batch[\"labels\"].view(-1) != tokenizer.pad_token_id\n",
    "        active_logits = logits.view(-1, model.config.vocab_size)[active_loss]\n",
    "        active_labels = batch[\"labels\"].view(-1)[active_loss]\n",
    "        loss = torch.nn.functional.cross_entropy(active_logits, active_labels)\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        preds_flatten = torch.flatten(predictions)\n",
    "        refs_flatten = torch.flatten(batch[\"labels\"])\n",
    "        metric.add_batch(predictions=preds_flatten, references=refs_flatten)\n",
    "        progress_bar_val.update(1)\n",
    "    average_loss = total_loss/total_batches\n",
    "    val_accuracy = metric.compute()\n",
    "    progress_bar_val.close()\n",
    "    return average_loss, val_accuracy['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b4c49813-316c-4978-9ec4-739d87cfd943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs):\n",
    "    progress_bar = tqdm(range(num_training_steps), unit=\"batch\")\n",
    "    early_stopping = EarlyStopping()\n",
    "    model.train()\n",
    "\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        total_batches = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"])\n",
    "            logits = outputs.logits\n",
    "            active_loss = batch[\"labels\"].view(-1) != tokenizer.pad_token_id\n",
    "            active_logits = logits.view(-1, model.config.vocab_size)[active_loss]\n",
    "            active_labels = batch[\"labels\"].view(-1)[active_loss]\n",
    "            loss = torch.nn.functional.cross_entropy(active_logits, active_labels)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            preds_flatten = torch.flatten(predictions)\n",
    "            refs_flatten = torch.flatten(batch[\"labels\"])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "            progress_bar.update(1)\n",
    "        train_loss = running_loss/total_batches\n",
    "        val_loss, val_acc = validate(model, eval_dataloader)\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'flan-T5-finetuned-dg-{}-run-context'.format(epoch + 11))\n",
    "        \n",
    "        print(\"Epoch {} : Training Loss {} Val Loss {} Val Acc {}%\".format(epoch + 1, train_loss, val_loss, val_acc))\n",
    "\n",
    "        history.append((train_loss, val_loss))\n",
    "        \n",
    "        early_stopping(train_loss, val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "          print(\"We are at epoch:\", epoch)\n",
    "          break\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f3e77-5a90-4e3d-9ea3-6a8bd3d79416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = train(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6955cf-17e6-4b66-b19c-116893eb8f2e",
   "metadata": {},
   "source": [
    "Early stopped manually at epoch 6, best model is at epoch 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4298b37b-cc7e-49cb-b804-f289ea52d612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.3074604242180627, 2.024561939140161),\n",
       " (1.8998920473379728, 1.9423575278509546),\n",
       " (1.833051116040974, 1.908637441970684),\n",
       " (1.7893838651089211, 1.884109921063538),\n",
       " (1.7561161939928704, 1.873801680488719),\n",
       " (1.729378885156181, 1.8638761584405545),\n",
       " (1.7094095301460868, 1.8599473062764715),\n",
       " (1.6943051865206784, 1.8569478474005505),\n",
       " (1.6838763919735178, 1.8558226124280028),\n",
       " (1.6778534264278337, 1.8550197614563837)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70cf66bb-7a2f-4ad4-b93d-d14acc19ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss, val_loss = zip(*history)\n",
    "\n",
    "plt.plot(train_loss, color = 'r')\n",
    "plt.plot(val_loss, color = 'b')\n",
    "\n",
    "# Optionally, add labels and a title\n",
    "plt.title('Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c888783c-96e4-4852-809b-44f262f20d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37fc8be14764474b4034d04fc1a58c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12830 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training Loss 2.0069628035938565 Val Loss 1.8550197614563837 Val Acc 0.2933666087962963%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 : Training Loss 1.6764463536551661 Val Loss 1.8550197614563837 Val Acc 0.2933666087962963%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflan-T5-finetuned-dg-10-run-context\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m model, history \u001b[38;5;241m=\u001b[39m train(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs)\n",
      "Cell \u001b[0;32mIn[137], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m preds_flatten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(predictions)\n\u001b[1;32m     22\u001b[0m refs_flatten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model.load_state_dict(torch.load('flan-T5-finetuned-dg-10-run-context')['model_state_dict'])\n",
    "\n",
    "model, history = train(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bda8daf9-fe48-4653-8b63-c233768632b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.2761684367122608, 0.2222222222222222, 0.0, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 2.3731489653819753,\n",
       " 'translation_length': 24199,\n",
       " 'reference_length': 10197}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    metric_bleu = evaluate.load(\"bleu\")\n",
    "    model.eval()\n",
    "    progress_bar_test = tqdm(range(len(test_dataloader)), leave=False)\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        preds_flatten = torch.flatten(predictions)\n",
    "        refs_flatten = torch.flatten(batch[\"labels\"])\n",
    "\n",
    "        preds_questions = tokenizer.batch_decode(preds_flatten, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        refs_questions = tokenizer.batch_decode(refs_flatten, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        metric_bleu.add_batch(predictions=preds_questions, references=refs_questions)\n",
    "        progress_bar_test.update(1)\n",
    "    test_bleu = metric_bleu.compute()\n",
    "    progress_bar_test.close()\n",
    "    return test_bleu\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model.load_state_dict(torch.load('flan-T5-finetuned-dg-12-run-context')['model_state_dict'])\n",
    "\n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "03fe96de-8fc5-456a-bc3d-915220743016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"How many mass extinctions have occurred throughout earth's history?\",\n",
       " 'distractor3': 'three',\n",
       " 'distractor1': 'six',\n",
       " 'distractor2': 'four',\n",
       " 'correct_answer': 'five',\n",
       " 'support': 'Extinction is the complete dying out of a species. Once a species goes extinct, it can never return. More than 99 percent of all the species that ever lived on Earth have gone extinct. Five mass extinctions have occurred in Earth’s history. They were caused by major geologic and climatic events. The fifth mass extinction wiped out the dinosaurs 65 million years ago.',\n",
       " 'context': 'Extinction is the complete dying out of a species. Once a species goes extinct, it can never return. More than 99 percent of all the species that ever lived on Earth have gone extinct. Five mass extinctions have occurred in Earth’s history. They were caused by major geologic and climatic events. The fifth mass extinction wiped out the dinosaurs 65 million years ago.'}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(model, question, answer, context):\n",
    "    text = \"Generate distractors for question: {}, answer: {}, context: {}\".format(question, answer, context)\n",
    "    max_length = 512\n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\").to(device) \n",
    "    decoder_input_ids = tokenized_inputs['input_ids']\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=tokenized_inputs['input_ids']\n",
    "    )\n",
    "    \n",
    "    distractors = tokenizer.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return distractors\n",
    "\n",
    "test = preprocessed_sciq['test'][800]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1146a0d2-64f3-43f4-9acc-02c069940d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Distractors: two, three, four']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(model, test['question'], test['correct_answer'], test['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2b919f5a-0e9c-4c22-9a70-06e3056a609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avertism', 'a symbiotic', 'a']\n",
      "{'question': 'Vertebrata are characterized by the presence of what?', 'distractor3': 'Thumbs', 'distractor1': 'Bones', 'distractor2': 'Muscles', 'correct_answer': 'backbone', 'support': 'Figure 29.7 Vertebrata are characterized by the presence of a backbone, such as the one that runs through the middle of this fish. All vertebrates are in the Craniata clade and have a cranium. (credit: Ernest V. More; taken at Smithsonian Museum of Natural History, Washington, D.', 'context': 'Figure 29.7 Vertebrata are characterized by the presence of a backbone, such as the one that runs through the middle of this fish. All vertebrates are in the Craniata clade and have a cranium. (credit: Ernest V. More; taken at Smithsonian Museum of Natural History, Washington, D.'}\n",
      "2\n",
      "['lungs', 'liver', 'kidney']\n",
      "{'question': 'Digestion of proteins begins with acids in what organ?', 'distractor3': 'brain', 'distractor1': 'colon', 'distractor2': 'liver', 'correct_answer': 'stomach', 'support': '24.4 Protein Metabolism Digestion of proteins begins in the stomach, where HCl and pepsin begin the process of breaking down proteins into their constituent amino acids. As the chyme enters the small intestine, it mixes with bicarbonate and digestive enzymes. The bicarbonate neutralizes the acidic HCl, and the digestive enzymes break down the proteins into smaller peptides and amino acids. Digestive hormones secretin and CCK are released from the small intestine to aid in digestive processes, and digestive proenzymes are released from the pancreas (trypsinogen and chymotrypsinogen). Enterokinase, an enzyme located in the wall of the small intestine, activates trypsin, which in turn activates chymotrypsin. These enzymes liberate the individual amino acids that are then transported via sodium-amino acid transporters across the intestinal wall into the cell. The amino acids are then transported into the bloodstream for dispersal to the liver and cells throughout the body to be used to create new proteins. When in excess, the amino acids are processed and stored as glucose or ketones. The nitrogen waste that is liberated in this process is converted to urea in the urea acid cycle and eliminated in the urine. In times of starvation, amino acids can be used as an energy source and processed through the Krebs cycle.', 'context': '24.4 Protein Metabolism Digestion of proteins begins in the stomach, where HCl and pepsin begin the process of breaking down proteins into their constituent amino acids. As the chyme enters the small intestine, it mixes with bicarbonate and digestive enzymes. The bicarbonate neutralizes the acidic HCl, and the digestive enzymes break down the proteins into smaller peptides and amino acids. Digestive hormones secretin and CCK are released from the small intestine to aid in digestive processes, and digestive proenzymes are released from the pancreas (trypsinogen and chymotrypsinogen). Enterokinase, an enzyme located in the wall of the small intestine, activates trypsin, which in turn activates chymotrypsin. These enzymes liberate the individual amino acids that are then transported via sodium-amino acid transporters across the intestinal wall into the cell. The amino acids are then transported into the bloodstream for dispersal to the liver and cells throughout the body to be used to create new proteins. When in excess, the amino acids are processed and stored as glucose or ketones. The nitrogen waste that is liberated in this process is converted to urea in the urea acid cycle and eliminated in the urine. In times of starvation, amino acids can be used as an energy source and processed through the Krebs cycle.'}\n",
      "27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count\n\u001b[0;32m---> 14\u001b[0m run_inference(test_data)\n",
      "Cell \u001b[0;32mIn[136], line 5\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(test_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_data):\n\u001b[0;32m----> 5\u001b[0m     distractors \u001b[38;5;241m=\u001b[39m inference(model, test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect_answer\u001b[39m\u001b[38;5;124m'\u001b[39m], test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     distractors_arr \u001b[38;5;241m=\u001b[39m distractors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     distractors_arr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m distractors_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistractors: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[131], line 7\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, question, answer, context)\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(text, max_length\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m      5\u001b[0m decoder_input_ids \u001b[38;5;241m=\u001b[39m tokenized_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      8\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mtokenized_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m distractors \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distractors\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[1;32m   1545\u001b[0m         input_ids,\n\u001b[1;32m   1546\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1547\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1548\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1549\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m   1550\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1551\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[1;32m   1552\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1553\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1554\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2406\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2407\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2408\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2409\u001b[0m )\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1748\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1748\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1749\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1750\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1751\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1752\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1753\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1754\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1755\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1756\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1757\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1758\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1759\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1760\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1761\u001b[0m )\n\u001b[1;32m   1763\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1115\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1101\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1102\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         output_attentions,\n\u001b[1;32m   1113\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m   1116\u001b[0m         hidden_states,\n\u001b[1;32m   1117\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1118\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m   1119\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1120\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1121\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1122\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1123\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1124\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m   1125\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1126\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1127\u001b[0m     )\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m0\u001b[39m](\n\u001b[1;32m    696\u001b[0m     hidden_states,\n\u001b[1;32m    697\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    698\u001b[0m     position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    699\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    700\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    701\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    702\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    703\u001b[0m )\n\u001b[1;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    705\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ):\n\u001b[0;32m--> 601\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m    602\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    603\u001b[0m         normed_hidden_states,\n\u001b[1;32m    604\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:254\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    255\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data = preprocessed_sciq['test']\n",
    "def run_inference(test_data):\n",
    "    count = 0\n",
    "    for idx, test in enumerate(test_data):\n",
    "        distractors = inference(model, test['question'], test['correct_answer'], test['context'])\n",
    "        distractors_arr = distractors[0].split(', ')\n",
    "        distractors_arr[0] = distractors_arr[0].replace('Distractors: ', '')\n",
    "        if len(distractors_arr) > 2 and distractors_arr[0] != distractors_arr[1] and distractors_arr[0] != distractors_arr[2] and distractors_arr[1] != distractors_arr[2]:\n",
    "            print(distractors_arr)\n",
    "            print(test_data[idx])\n",
    "            print(idx)\n",
    "            count += 1\n",
    "    return count\n",
    "run_inference(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2de155d8-37b1-461c-9bc2-8027c630c1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93496eb6c144d38a32d4b6b63a1af42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "656475d9-739d-4b76-9f3a-9af86b73b153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a4f277aba84ee588a3a17fe37e7c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rizkiduwinanto/distractor-generation/commit/ca9be0bd6cb9c1fe4420a17cd5e121ebd31c61ed', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='ca9be0bd6cb9c1fe4420a17cd5e121ebd31c61ed', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"distractor-generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
