{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766612d2-7bb3-4c70-bf16-83f1adcc99a8",
   "metadata": {},
   "source": [
    "## Pipeline Quiz Generator (Separate Quiz and Distractor Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413f9cf-5025-423f-bf1b-4eb3fa900697",
   "metadata": {},
   "source": [
    "Description: Quiz Generator with separate pipeline for quiz generation and then distractor generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbacf58-f521-404d-af8d-8092a24a441f",
   "metadata": {},
   "source": [
    "### Step 1 : SciQ Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19678d2d-152b-4f66-ac5f-b2fea2fc0bc0",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a5f69ea9-2967-40f6-9018-1cf30b35c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq_dataset = load_dataset(\"allenai/sciq\")\n",
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03e40d-09c8-4479-8f7a-66af4fa73200",
   "metadata": {},
   "source": [
    "Sample Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1741ece6-e8cb-4a10-9ad5-f1d2c54cac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A small scale version of what type of map displays individual rock units?',\n",
       " 'distractor3': 'polar map',\n",
       " 'distractor1': 'seismic map',\n",
       " 'distractor2': 'geographic map',\n",
       " 'correct_answer': 'geologic map',\n",
       " 'support': 'Geologic maps display rock units and geologic features. A small scale map displays individual rock units while a large scale map shows geologic provinces.'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset[\"train\"][27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf6c12-c1e6-4f2f-90f6-230ac55feb72",
   "metadata": {},
   "source": [
    "Drop every data with empty support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37af7ae3-1aa7-478a-be68-4176effd5198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sciq = sciq_dataset.filter(lambda example: example[\"support\"] != '')\n",
    "filtered_sciq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db998f-1e85-47ad-a80c-91a06397562a",
   "metadata": {},
   "source": [
    "Check for support with longer than 512 tokens/words (Maximum token of T5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fcdbc4a6-7595-41ff-88c0-189c8d0bc9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>distractor3</th>\n",
       "      <th>distractor1</th>\n",
       "      <th>distractor2</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>viruses</td>\n",
       "      <td>protozoa</td>\n",
       "      <td>gymnosperms</td>\n",
       "      <td>mesophilic organisms</td>\n",
       "      <td>Mesophiles grow best in moderate temperature, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>tropical effect</td>\n",
       "      <td>muon effect</td>\n",
       "      <td>centrifugal effect</td>\n",
       "      <td>coriolis effect</td>\n",
       "      <td>Without Coriolis Effect the global winds would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>endothermic</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>reactive</td>\n",
       "      <td>exothermic</td>\n",
       "      <td>Summary Changes of state are examples of phase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>zeta decay</td>\n",
       "      <td>beta decay</td>\n",
       "      <td>gamma decay</td>\n",
       "      <td>alpha decay</td>\n",
       "      <td>All radioactive decay is dangerous to living t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the world’s most continuo...</td>\n",
       "      <td>magma</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>carbon and smog</td>\n",
       "      <td>smoke and ash</td>\n",
       "      <td>Example 3.5 Calculating Projectile Motion: Hot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      distractor3  \\\n",
       "0  What type of organism is commonly used in prep...          viruses   \n",
       "1  What phenomenon makes global winds blow northe...  tropical effect   \n",
       "2  Changes from a less-ordered state to a more-or...      endothermic   \n",
       "3     What is the least dangerous radioactive decay?       zeta decay   \n",
       "4  Kilauea in hawaii is the world’s most continuo...            magma   \n",
       "\n",
       "        distractor1         distractor2        correct_answer  \\\n",
       "0          protozoa         gymnosperms  mesophilic organisms   \n",
       "1       muon effect  centrifugal effect       coriolis effect   \n",
       "2        unbalanced            reactive            exothermic   \n",
       "3        beta decay         gamma decay           alpha decay   \n",
       "4  greenhouse gases     carbon and smog         smoke and ash   \n",
       "\n",
       "                                             support  \n",
       "0  Mesophiles grow best in moderate temperature, ...  \n",
       "1  Without Coriolis Effect the global winds would...  \n",
       "2  Summary Changes of state are examples of phase...  \n",
       "3  All radioactive decay is dangerous to living t...  \n",
       "4  Example 3.5 Calculating Projectile Motion: Hot...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(sciq_dataset[\"train\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ee8ef979-2b27-49ee-83ae-ee66f3ea774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3559\n"
     ]
    }
   ],
   "source": [
    "print(df_train['support'].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2156e61-1202-4df3-85c7-5893eb017c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    }
   ],
   "source": [
    "print(df_train['question'].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aab855a-8a1b-4cf9-bc0f-d8a5a9e4e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = filtered_sciq.filter(lambda example: len(example[\"support\"]) > 3000) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "769ade74-e3fa-47ee-902d-e5a5f9dcea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'membrane gradients was known, Mitchell proposed that energy captured through the absorption of light (by phototrophs) or the breakdown of molecules into more stable molecules (by various types of chemotrophs) relied on the same basic (homologous) mechanism, namely the generation of H+ gradients across membranes (the plasma membrane in prokaryotes or the internal membranes of mitochondria or chloroplasts (intracellular organelles, derived from bacteria – see below) in eukaryotes. What makes us think that these processes might have a similar evolutionary root, that they are homologous? Basically, it is the observation that in both light- and chemical-based processes captured energy is transferred through the movement of electrons through a membrane-embedded “electron transport chain”. An electron transport chain involves a series of membrane and associated proteins and a series of reduction-oxidation or redox reactions (see below) during which electrons move from a high energy donor to a lower energy acceptor. Some of the energy difference between the two is used to move H+ ions across a membrane, generating a H+ concentration gradient. Subsequently the thermodynamically favorable movement of H+ down this concentration gradient (across the membrane) is used to drive ATP synthesis, a thermodynamically unfavorable process. ATP synthesis itself involves the rotating ATP synthase. The reaction can be written: H+outside + ADP + Pi ATP + H2O + H+inside, where “inside” and “outside” refer to compartments defined by the membrane containing the electron transport chain and the ATP synthase. Again, this reaction can run backwards. When this occurs, the ATP synthase acts as an ATPase (ATP hydrolase) that can pump H+ (or other molecules) against its concentration gradient. Such pumping ATPases establishes most biologically important molecular gradients across membranes. In such a reaction: ATP + H2O + molecule in low concentration region ADP + Pi + molecule in low concentration region. The most important difference between phototrophs and chemotrophs is how high energy electrons enter the electron transport chain. Oxygenic photosynthesis \\u2028 Compared to the salt loving archaea Halobium with its purple bacteriorhodopin-rich membranes, photosynthetic cyanobacteria (which are true bacteria), green algae, and higher plants (both eukaryotes) use more complex molecular systems through which to capture and utilize light. In all of these organisms, their photosynthetic systems appear to be homologous, that is derived from a common ancestor, a topic we will return to later in this chapter. For simplicity’s sake we will describe the photosynthetic system of cyanobacterium; the system in eukaryotic algae and plants, while more complex, follows the same basic logic. At this point, we consider only one aspect of this photosynthetic system, known as the oxygenic or non-cyclic system (look to more advanced classes for more details. ) The major pigment in this system, chlorophyll, is based on a complex molecule, a porphyrin (see above) and it is primarily these pigments that give plants their green color. As in the case of retinal, they absorb visible light due to the presence of a conjugated bonding structure (drawn as a series of alternating single and double) carbon-carbon bonds. Chlorophyll is synthesized by a conserved biosynthetic pathway that is also used to synthesize heme, which is found in the hemoglobin of animals and in the cytochromes, within the electron transport chain present in both plants and animals (which.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['train'][7]\n",
    "test_data['support']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e924e3-6f09-44b2-8b96-857c53343fe2",
   "metadata": {},
   "source": [
    "We can see above that support is long but only a few sentences is relevant, we cannot do raw summarization, we have to extract text based on keywords which are answers (distractors and keywords from questions too!). If we left this, support and answer will be truncated. If we summarize it raw, we lose important info of what is asked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd5cb9-928f-4a20-b7b4-fc5b56ff8d98",
   "metadata": {},
   "source": [
    "Extractive Summarization based on answer and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c795ce16-5af8-42dd-a38c-68d2e76b7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_question(question):\n",
    "    kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(question)\n",
    "    return [keyword for keyword, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97d14d86-d15a-4bb3-8286-a6e63403c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok for tok in doc]\n",
    "    lemmas = [tok.lemma_ for tok in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e0fe6f8-2d14-4b5b-a59c-e09ee3a989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence, words):\n",
    "    score = 0\n",
    "    clean_sentences = clean_text(sentence.lower())\n",
    "    for word in words:\n",
    "        if clean_text(word.lower()) in clean_sentences:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f88fa3c-f069-46d1-8311-4987a46119b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ) The major pigment in this system, chlorophyll, is based on a complex molecule, a porphyrin (see above) and it is primarily these pigments that give plants their green color. At this point, we consider only one aspect of this photosynthetic system, known as the oxygenic or non-cyclic system (look to more advanced classes for more details. Chlorophyll is synthesized by a conserved biosynthetic pathway that is also used to synthesize heme, which is found in the hemoglobin of animals and in the cytochromes, within the electron transport chain present in both plants and animals (which. For simplicity’s sake we will describe the photosynthetic system of cyanobacterium; the system in eukaryotic algae and plants, while more complex, follows the same basic logic. In all of these organisms, their photosynthetic systems appear to be homologous, that is derived from a common ancestor, a topic we will return to later in this chapter. Oxygenic photosynthesis \\u2028 Compared to the salt loving archaea Halobium with its purple bacteriorhodopin-rich membranes, photosynthetic cyanobacteria (which are true bacteria), green algae, and higher plants (both eukaryotes) use more complex molecular systems through which to capture and utilize light'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_support(example, max_words=256):\n",
    "    text = example[\"support\"]\n",
    "    words = extract_question(example[\"question\"])\n",
    "    words.extend([test_data[\"correct_answer\"]])\n",
    "\n",
    "    scored_sentences = (\n",
    "        (i, sentence, score_sentence(sentence, words))\n",
    "        for i, sentence in enumerate(text.split(\".\"))\n",
    "        if any(clean_text(w.lower()) in clean_text(sentence.lower()) for w in words)\n",
    "    )\n",
    "    ranked_sentences = sorted(scored_sentences, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    sentence_in_summary = []\n",
    "    sum_of_words = 0\n",
    "    for order, sentence, _ in ranked_sentences:\n",
    "        num_of_words = len(sentence.split())\n",
    "        if sum_of_words + num_of_words < max_words:\n",
    "            sentence_in_summary.append((order, sentence))\n",
    "            sum_of_words += num_of_words \n",
    "\n",
    "    summary = sorted(sentence_in_summary, key=lambda x: x[1])\n",
    "    return \".\".join(sent for _, sent in summary)\n",
    "\n",
    "\n",
    "summarize_support(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0684631-ea06-4391-bb96-e58d874d5b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916980ecabe146a187527c21a947fc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c40cc3a6564d44a88c3056b28c0742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fe32fe14cd4246a44ae6db1cee294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_context(example, max_token_size=256):\n",
    "    answer_size = len(example['correct_answer'].split())\n",
    "    support_size = len(example['support'].split())\n",
    "    words_len = answer_size + support_size + 1\n",
    "    context = example['support']\n",
    "\n",
    "    if words_len > max_token_size:\n",
    "        max_new_token_size = max_token_size - answer_size - 1\n",
    "        context = summarize_support(example, max_words=max_new_token_size)\n",
    "    \n",
    "    return {\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "preprocessed_sciq = filtered_sciq.map(generate_context, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b1823146-ef39-41fe-a6a0-8b51bfc115f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = preprocessed_sciq.filter(lambda example: len(example[\"question\"].split()) > 64) \n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3473d4d4-2376-4654-b0cc-17b2c659fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'The ideal gas law is used like any other gas law, with attention paid to the unit and making sure that temperature is expressed in kelvin. however, the ideal gas law does not require a change in the conditions of a gas sample. the ideal gas law implies that if you know any three of the physical properties of a gas, you can calculate this?',\n",
       " 'distractor3': 'unrelated',\n",
       " 'distractor1': 'second',\n",
       " 'distractor2': 'third',\n",
       " 'correct_answer': 'fourth',\n",
       " 'support': 'The ideal gas law is used like any other gas law, with attention paid to the unit and making sure that temperature is expressed in Kelvin. However, the ideal gas law does not require a change in the conditions of a gas sample. The ideal gas law implies that if you know any three of the physical properties of a gas, you can calculate the fourth property.',\n",
       " 'context': 'The ideal gas law is used like any other gas law, with attention paid to the unit and making sure that temperature is expressed in Kelvin. However, the ideal gas law does not require a change in the conditions of a gas sample. The ideal gas law implies that if you know any three of the physical properties of a gas, you can calculate the fourth property.'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fdd2a16a-80be-47c6-807a-1eeb1f2bac8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d856503587d48ce853c05d7ab2ac062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4708e6f23f431da35274a3a5a76ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b961f520294183b094f811dc23cfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_sciq.save_to_disk(\"preprocessed_sciq-qg-256-new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4e16d-63b7-4ce8-980d-d4b5a8c04efe",
   "metadata": {},
   "source": [
    "Now preprocessed has shorter context for long ones, due to map process running slow, uncomment below after getting zip file from me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e5bf3694-b698-47c6-a91d-6aea4443544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# preprocessed_sciq = load_from_disk(\"preprocessed_sciq-qg-256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3020c7f-3b2e-4634-b55b-62dde2253b1d",
   "metadata": {},
   "source": [
    "### Step 2 Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c47179-e1f2-48f5-8e72-2de0111700d8",
   "metadata": {},
   "source": [
    "#### Tokenize for Question Generation \n",
    "\n",
    "Input : Context and Answer \\\n",
    "Output : Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7cd6ea20-1618-49b8-a276-a5bc697ffefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8c730acd-e5d0-4544-9caa-b4113c3838ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   37,  1523,  1807,   973,    19,   261,   114,   136,   119,  1807,\n",
       "           973,     6,    28,  1388,  1866,    12,     8,  1745,    11,   492,\n",
       "           417,    24,  2912,    19,  7103,    16,     3,  5768,  2494,     5,\n",
       "           983,     6,     8,  1523,  1807,   973,   405,    59,  1457,     3,\n",
       "             9,   483,    16,     8,  1124,    13,     3,     9,  1807,  3106,\n",
       "             5,     8,  1523,  1807,   973, 18841,    24,     3,    99,    25,\n",
       "           214,   136,   386,    13,     8,  1722,  2605,    13,     3,     9,\n",
       "          1807,     6,    25,    54, 11837,    48,    58,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = test_x['train'][0]['question']\n",
    "tokenized_targets = tokenizer.encode_plus(question, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "tokenized_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "90b92480-d5d3-40e8-bc35-6a4a198ab854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ideal gas law is used like any other gas law, with attention paid to the unit and making sure that temperature is expressed in kelvin. however, the ideal gas law does not require a change in the conditions of a gas sample. the ideal gas law implies that if you know any three of the physical properties of a gas, you can calculate this?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_targets['input_ids'][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3316e356-7e22-4385-9dba-e395bb0b890e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb70eede102948bab702130d6570e456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd881beea9d4ed08adcab84507614dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ec716cae4a457a9ee3559471286636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(example):\n",
    "    text = \"{}<sep>{}\".format(example['correct_answer'], example['context'])\n",
    "    question = example['question']\n",
    "\n",
    "    max_length = 256\n",
    "    max_length_target=64\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(question, max_length=max_length_target, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset = preprocessed_sciq.map(preprocess_dataset, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "79cb7c31-3517-464d-a4bc-d1d7cb407950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 13:20:32.725572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-25 13:20:32.725713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-25 13:20:32.814307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-25 13:20:33.046298: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-25 13:20:34.397176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"bleu\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9578193a-0133-4cc9-88e1-a770bc95e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pretrained_question_generation\", \n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    "    # gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "924faebf-8d6c-4c34-a225-b3d91192bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdc93e-fb24-4690-92df-b4834b850940",
   "metadata": {},
   "source": [
    "There is no validation yet (it's buggy in my comp for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d590270d-838c-4883-a60a-432c5ee502ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='1965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 656/1965 21:56 < 43:54, 0.50 it/s, Epoch 1.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='369' max='887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [369/887 11:17 < 15:53, 0.54 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory pretrained_question_generation/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='3930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  64/3930 01:04 < 1:06:50, 0.96 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1625\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1626\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1627\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1628\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1629\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/utils/memory.py:136\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(batch_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2911\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1966\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6207499c-fd38-4fa8-af43-3c5105e1dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model-qg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0448974-f855-4242-b0ff-c24f0cb01ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [11499,\n",
       "  32100,\n",
       "  37,\n",
       "  2677,\n",
       "  5013,\n",
       "  2107,\n",
       "  387,\n",
       "  45,\n",
       "  3,\n",
       "  9,\n",
       "  6957,\n",
       "  42,\n",
       "  13064,\n",
       "  616,\n",
       "  190,\n",
       "  6079,\n",
       "  9243,\n",
       "  7293,\n",
       "  7,\n",
       "  6,\n",
       "  114,\n",
       "  273,\n",
       "  16,\n",
       "  7996,\n",
       "  666,\n",
       "  3,\n",
       "  5,\n",
       "  100,\n",
       "  19,\n",
       "  2953,\n",
       "  57,\n",
       "  579,\n",
       "  2677,\n",
       "  11,\n",
       "  23295,\n",
       "  24,\n",
       "  169,\n",
       "  8,\n",
       "  387,\n",
       "  12,\n",
       "  1633,\n",
       "  70,\n",
       "  4096,\n",
       "  5,\n",
       "  634,\n",
       "  52,\n",
       "  1982,\n",
       "  10441,\n",
       "  19,\n",
       "  10441,\n",
       "  24,\n",
       "  3033,\n",
       "  7,\n",
       "  8,\n",
       "  2912,\n",
       "  13,\n",
       "  387,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [366,\n",
       "  8,\n",
       "  2912,\n",
       "  13,\n",
       "  387,\n",
       "  19,\n",
       "  1936,\n",
       "  227,\n",
       "  271,\n",
       "  261,\n",
       "  16,\n",
       "  9243,\n",
       "  6,\n",
       "  34,\n",
       "  19,\n",
       "  48,\n",
       "  607,\n",
       "  13,\n",
       "  10441,\n",
       "  58,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0878dabd-96e9-4dad-bc00-f8ed656e6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"model-qg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "72a5ee4c-cc30-47da-8a4e-d19c2c487335",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"{}<sep>{}\".format('minute amounts', \"Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap.\")\n",
    "tokenized_inputs = tokenizer.encode_plus(text, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a76f73ad-4d26-498f-9d65-1eaa0c961472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  363,   19,    8, 1657,   21,    3,    9, 6567,   29,    6, 6567,\n",
       "           29,   15,    6,   42, 9077, 2358,   24, 1472]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    input_ids=tokenized_inputs['input_ids']\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b5489e6c-e454-4a85-86ae-927611171354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>What is the term for a neuron, neurone, or nerve cell that fire\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ab1deaa-239a-427e-90f3-5def87964780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When the temperature of water is increased after being used in cooling, it is this form of pollution?',\n",
       " 'distractor3': 'air',\n",
       " 'distractor1': 'atmospheric',\n",
       " 'distractor2': 'cosmic',\n",
       " 'correct_answer': 'thermal',\n",
       " 'support': \"Thermal pollution is pollution that raises the temperature of water. This is caused by power plants and factories that use the water to cool their machines. The plants pump cold water from a lake or coastal area through giant cooling towers, like those in Figure below . As it flows through the towers, the cold water absorbs heat. This warmed water is returned to the lake or sea. Thermal pollution can kill fish and other water life. It's not just the warm temperature that kills them. Warm water can’t hold as much oxygen as cool water. If the water gets too warm, there may not be enough oxygen for living things.\",\n",
       " 'context': ' The plants pump cold water from a lake or coastal area through giant cooling towers, like those in Figure below . This is caused by power plants and factories that use the water to cool their machines.Thermal pollution is pollution that raises the temperature of water'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sciq['validation'][137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "762c05e2-9a7c-4366-93ae-e346a5924dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in tokenized_dataset['validation'][137]['input_ids']:\n",
    "#     print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6c7a8ca-1546-4762-b75b-da4c84480dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/887 04:17 < 23:05, 0.54 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.32 GiB (GPU 0; 6.00 GiB total capacity; 5.98 GiB already allocated; 0 bytes free; 8.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   3230\u001b[0m     eval_dataloader,\n\u001b[1;32m   3231\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3232\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   3233\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   3234\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3235\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   3236\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   3237\u001b[0m )\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3444\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3442\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3443\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m-> 3444\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(preds_host, logits, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m   3446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3447\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:125\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    128\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:84\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     81\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((tensor1, tensor2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     87\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.32 GiB (GPU 0; 6.00 GiB total capacity; 5.98 GiB already allocated; 0 bytes free; 8.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4d657-fd13-47c3-be87-a2633626c43e",
   "metadata": {},
   "source": [
    "### Step 3 Distractor Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f36d75-49b1-45ef-b1f9-a8db16698a92",
   "metadata": {},
   "source": [
    "#### Tokenize for Distractor Generation \n",
    "\n",
    "Input : Answer, Question, Context \\\n",
    "Output : 3 Distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2ed2d55-ff0e-417b-825f-833f8ab35c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2148cf41ff894863b06440cb46e336f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afaa3159db7649af90e26640d93292fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32784d6a1c3d4b29899249cddeedb73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset_for_distractor(example):\n",
    "    text = \"{}<sep>{}<sep>{}\".format(example['question'], example['correct_answer'], example['context'])\n",
    "    distractor = \"{}<sep>{}<sep>{}\".format(example['distractor1'], example['distractor2'], example['distractor3'])\n",
    "\n",
    "    max_length = 256\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(distractor, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset_distractor = preprocessed_sciq.map(preprocess_dataset_for_distractor, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "73d38267-cb3e-4309-bcfd-0c1c2f411e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_dis = TrainingArguments(\n",
    "    output_dir=\"pretrained_distractor_generation\", \n",
    "    evaluation_strategy=\"no\", \n",
    "    logging_strategy=\"epoch\",\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True\n",
    "    # gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "model_dis = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_dis = model_dis.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_dis,\n",
    "    args=training_args_dis,\n",
    "    train_dataset=tokenized_dataset_distractor[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_distractor[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d812e71-345e-4343-9fac-6eac2bf4204e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1965' max='1965' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1965/1965 1:45:09, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>1.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1311</td>\n",
       "      <td>1.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>1.661900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1965, training_loss=1.7532189910345102, metrics={'train_runtime': 6312.5134, 'train_samples_per_second': 4.981, 'train_steps_per_second': 0.311, 'total_flos': 2126625726529536.0, 'train_loss': 1.7532189910345102, 'epoch': 3.0})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "27346c6f-03ac-439d-ba03-bc171864e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model-dg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1af6cf19-5479-4f9d-b008-c0729667025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dis = T5ForConditionalGeneration.from_pretrained(\"model-dg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9d4f35c3-b3cb-4546-b1d5-88e29f1492bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Each species has a particular way of making a living which is called its what?',\n",
       " 'distractor3': 'life-cycle',\n",
       " 'distractor1': 'habit',\n",
       " 'distractor2': 'system',\n",
       " 'correct_answer': 'niche',\n",
       " 'support': 'Each species has a particular way of making a living. This is called its niche . You can see the niche of a lion in Figure below . A lion makes its living by hunting and eating other animals. Each species also has a certain place where it is best suited to live. This is called its habitat . The lion’s habitat is a grassland. Why is a lion better off in a grassland than in a forest?.',\n",
       " 'context': ' A lion makes its living by hunting and eating other animals. Each species also has a certain place where it is best suited to live. This is called its habitat . This is called its niche .Each species has a particular way of making a living'}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = preprocessed_sciq['test'][700]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1bd4002e-e0d0-4268-a416-8cfcc1b7144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"{}<sep>{}<sep>{}\".format(test_data['question'], test_data['correct_answer'], test_data['context'])\n",
    "tokenized_inputs = tokenizer.encode_plus(text, max_length=256, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a133623b-110f-4b00-bc73-1af434db989f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    3,    7,  232,    2,    7,   15,  102, 3155,    7,   15,    9,\n",
       "            2,    7,   15,  102, 3155,    7,   15,    9]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model_dis.generate(\n",
    "    input_ids=tokenized_inputs['input_ids']\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c762ab87-d6fe-4b0b-b197-831c3f690ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> sand<unk> sep>sea<unk>sep>sea\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
