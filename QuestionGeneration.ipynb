{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766612d2-7bb3-4c70-bf16-83f1adcc99a8",
   "metadata": {},
   "source": [
    "## Pipeline Quiz Generator (Separate Quiz and Distractor Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413f9cf-5025-423f-bf1b-4eb3fa900697",
   "metadata": {},
   "source": [
    "Description: Quiz Generator with separate pipeline for quiz generation and then distractor generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbacf58-f521-404d-af8d-8092a24a441f",
   "metadata": {},
   "source": [
    "### Step 1 : SciQ Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19678d2d-152b-4f66-ac5f-b2fea2fc0bc0",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f69ea9-2967-40f6-9018-1cf30b35c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sciq_dataset = load_dataset(\"allenai/sciq\")\n",
    "sciq_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03e40d-09c8-4479-8f7a-66af4fa73200",
   "metadata": {},
   "source": [
    "Sample Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1741ece6-e8cb-4a10-9ad5-f1d2c54cac24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A small scale version of what type of map displays individual rock units?',\n",
       " 'distractor3': 'polar map',\n",
       " 'distractor1': 'seismic map',\n",
       " 'distractor2': 'geographic map',\n",
       " 'correct_answer': 'geologic map',\n",
       " 'support': 'Geologic maps display rock units and geologic features. A small scale map displays individual rock units while a large scale map shows geologic provinces.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciq_dataset[\"train\"][27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf6c12-c1e6-4f2f-90f6-230ac55feb72",
   "metadata": {},
   "source": [
    "Drop every data with empty support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37af7ae3-1aa7-478a-be68-4176effd5198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10481\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 884\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sciq = sciq_dataset.filter(lambda example: example[\"support\"] != '')\n",
    "filtered_sciq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db998f-1e85-47ad-a80c-91a06397562a",
   "metadata": {},
   "source": [
    "Check for support with longer than 512 tokens/words (Maximum token of T5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdbc4a6-7595-41ff-88c0-189c8d0bc9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>distractor3</th>\n",
       "      <th>distractor1</th>\n",
       "      <th>distractor2</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>viruses</td>\n",
       "      <td>protozoa</td>\n",
       "      <td>gymnosperms</td>\n",
       "      <td>mesophilic organisms</td>\n",
       "      <td>Mesophiles grow best in moderate temperature, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>tropical effect</td>\n",
       "      <td>muon effect</td>\n",
       "      <td>centrifugal effect</td>\n",
       "      <td>coriolis effect</td>\n",
       "      <td>Without Coriolis Effect the global winds would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>endothermic</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>reactive</td>\n",
       "      <td>exothermic</td>\n",
       "      <td>Summary Changes of state are examples of phase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>zeta decay</td>\n",
       "      <td>beta decay</td>\n",
       "      <td>gamma decay</td>\n",
       "      <td>alpha decay</td>\n",
       "      <td>All radioactive decay is dangerous to living t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the world’s most continuo...</td>\n",
       "      <td>magma</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>carbon and smog</td>\n",
       "      <td>smoke and ash</td>\n",
       "      <td>Example 3.5 Calculating Projectile Motion: Hot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      distractor3  \\\n",
       "0  What type of organism is commonly used in prep...          viruses   \n",
       "1  What phenomenon makes global winds blow northe...  tropical effect   \n",
       "2  Changes from a less-ordered state to a more-or...      endothermic   \n",
       "3     What is the least dangerous radioactive decay?       zeta decay   \n",
       "4  Kilauea in hawaii is the world’s most continuo...            magma   \n",
       "\n",
       "        distractor1         distractor2        correct_answer  \\\n",
       "0          protozoa         gymnosperms  mesophilic organisms   \n",
       "1       muon effect  centrifugal effect       coriolis effect   \n",
       "2        unbalanced            reactive            exothermic   \n",
       "3        beta decay         gamma decay           alpha decay   \n",
       "4  greenhouse gases     carbon and smog         smoke and ash   \n",
       "\n",
       "                                             support  \n",
       "0  Mesophiles grow best in moderate temperature, ...  \n",
       "1  Without Coriolis Effect the global winds would...  \n",
       "2  Summary Changes of state are examples of phase...  \n",
       "3  All radioactive decay is dangerous to living t...  \n",
       "4  Example 3.5 Calculating Projectile Motion: Hot...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(sciq_dataset[\"train\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8ef979-2b27-49ee-83ae-ee66f3ea774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3559\n"
     ]
    }
   ],
   "source": [
    "print(df_train['support'].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aab855a-8a1b-4cf9-bc0f-d8a5a9e4e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 0\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = filtered_sciq.filter(lambda example: len(example[\"support\"]) > 3000) \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769ade74-e3fa-47ee-902d-e5a5f9dcea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'membrane gradients was known, Mitchell proposed that energy captured through the absorption of light (by phototrophs) or the breakdown of molecules into more stable molecules (by various types of chemotrophs) relied on the same basic (homologous) mechanism, namely the generation of H+ gradients across membranes (the plasma membrane in prokaryotes or the internal membranes of mitochondria or chloroplasts (intracellular organelles, derived from bacteria – see below) in eukaryotes. What makes us think that these processes might have a similar evolutionary root, that they are homologous? Basically, it is the observation that in both light- and chemical-based processes captured energy is transferred through the movement of electrons through a membrane-embedded “electron transport chain”. An electron transport chain involves a series of membrane and associated proteins and a series of reduction-oxidation or redox reactions (see below) during which electrons move from a high energy donor to a lower energy acceptor. Some of the energy difference between the two is used to move H+ ions across a membrane, generating a H+ concentration gradient. Subsequently the thermodynamically favorable movement of H+ down this concentration gradient (across the membrane) is used to drive ATP synthesis, a thermodynamically unfavorable process. ATP synthesis itself involves the rotating ATP synthase. The reaction can be written: H+outside + ADP + Pi ATP + H2O + H+inside, where “inside” and “outside” refer to compartments defined by the membrane containing the electron transport chain and the ATP synthase. Again, this reaction can run backwards. When this occurs, the ATP synthase acts as an ATPase (ATP hydrolase) that can pump H+ (or other molecules) against its concentration gradient. Such pumping ATPases establishes most biologically important molecular gradients across membranes. In such a reaction: ATP + H2O + molecule in low concentration region ADP + Pi + molecule in low concentration region. The most important difference between phototrophs and chemotrophs is how high energy electrons enter the electron transport chain. Oxygenic photosynthesis \\u2028 Compared to the salt loving archaea Halobium with its purple bacteriorhodopin-rich membranes, photosynthetic cyanobacteria (which are true bacteria), green algae, and higher plants (both eukaryotes) use more complex molecular systems through which to capture and utilize light. In all of these organisms, their photosynthetic systems appear to be homologous, that is derived from a common ancestor, a topic we will return to later in this chapter. For simplicity’s sake we will describe the photosynthetic system of cyanobacterium; the system in eukaryotic algae and plants, while more complex, follows the same basic logic. At this point, we consider only one aspect of this photosynthetic system, known as the oxygenic or non-cyclic system (look to more advanced classes for more details. ) The major pigment in this system, chlorophyll, is based on a complex molecule, a porphyrin (see above) and it is primarily these pigments that give plants their green color. As in the case of retinal, they absorb visible light due to the presence of a conjugated bonding structure (drawn as a series of alternating single and double) carbon-carbon bonds. Chlorophyll is synthesized by a conserved biosynthetic pathway that is also used to synthesize heme, which is found in the hemoglobin of animals and in the cytochromes, within the electron transport chain present in both plants and animals (which.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['train'][7]\n",
    "test_data['support']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e924e3-6f09-44b2-8b96-857c53343fe2",
   "metadata": {},
   "source": [
    "We can see above that support is long but only a few sentences is relevant, we cannot do raw summarization, we have to extract text based on keywords which are answers (distractors and keywords from questions too!). If we left this, support and answer will be truncated. If we summarize it raw, we lose important info of what is asked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd5cb9-928f-4a20-b7b4-fc5b56ff8d98",
   "metadata": {},
   "source": [
    "Extractive Summarization based on answer and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c795ce16-5af8-42dd-a38c-68d2e76b7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "\n",
    "def extract_question(question):\n",
    "    kw_extractor = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "    keywords = kw_extractor.extract_keywords(question)\n",
    "    return [keyword for keyword, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d14d86-d15a-4bb3-8286-a6e63403c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok for tok in doc]\n",
    "    lemmas = [tok.lemma_ for tok in tokens]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0fe6f8-2d14-4b5b-a59c-e09ee3a989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence, words):\n",
    "    score = 0\n",
    "    clean_sentences = clean_text(sentence.lower())\n",
    "    for word in words:\n",
    "        if clean_text(word.lower()) in clean_sentences:\n",
    "            score += 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f88fa3c-f069-46d1-8311-4987a46119b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ) The major pigment in this system, chlorophyll, is based on a complex molecule, a porphyrin (see above) and it is primarily these pigments that give plants their green color. At this point, we consider only one aspect of this photosynthetic system, known as the oxygenic or non-cyclic system (look to more advanced classes for more details. Chlorophyll is synthesized by a conserved biosynthetic pathway that is also used to synthesize heme, which is found in the hemoglobin of animals and in the cytochromes, within the electron transport chain present in both plants and animals (which. For simplicity’s sake we will describe the photosynthetic system of cyanobacterium; the system in eukaryotic algae and plants, while more complex, follows the same basic logic. In all of these organisms, their photosynthetic systems appear to be homologous, that is derived from a common ancestor, a topic we will return to later in this chapter. Oxygenic photosynthesis \\u2028 Compared to the salt loving archaea Halobium with its purple bacteriorhodopin-rich membranes, photosynthetic cyanobacteria (which are true bacteria), green algae, and higher plants (both eukaryotes) use more complex molecular systems through which to capture and utilize light'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_support(example, max_words=256):\n",
    "    text = example[\"support\"]\n",
    "    words = extract_question(example[\"question\"])\n",
    "    words.extend([test_data[\"correct_answer\"]])\n",
    "\n",
    "    scored_sentences = (\n",
    "        (i, sentence, score_sentence(sentence, words))\n",
    "        for i, sentence in enumerate(text.split(\".\"))\n",
    "        if any(clean_text(w.lower()) in clean_text(sentence.lower()) for w in words)\n",
    "    )\n",
    "    ranked_sentences = sorted(scored_sentences, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    sentence_in_summary = []\n",
    "    sum_of_words = 0\n",
    "    for order, sentence, _ in ranked_sentences:\n",
    "        num_of_words = len(sentence.split())\n",
    "        if sum_of_words + num_of_words < max_words:\n",
    "            sentence_in_summary.append((order, sentence))\n",
    "            sum_of_words += num_of_words \n",
    "\n",
    "    summary = sorted(sentence_in_summary, key=lambda x: x[1])\n",
    "    return \".\".join(sent for _, sent in summary)\n",
    "\n",
    "\n",
    "summarize_support(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0684631-ea06-4391-bb96-e58d874d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(example, max_token_size=256):\n",
    "    answer_size = len(example['correct_answer'].split())\n",
    "    support_size = len(example['support'].split())\n",
    "    words_len = answer_size + support_size + 1\n",
    "    context = example['support']\n",
    "\n",
    "    if words_len > max_token_size:\n",
    "        max_new_token_size = max_token_size - answer_size - 1\n",
    "        context = summarize_support(example, max_words=max_new_token_size)\n",
    "    \n",
    "    return {\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "# preprocessed_sciq = filtered_sciq.map(generate_context, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1823146-ef39-41fe-a6a0-8b51bfc115f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_x = preprocessed_sciq.filter(lambda example: len(example[\"question\"].split()) > 64) \n",
    "# test_x['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd2a16a-80be-47c6-807a-1eeb1f2bac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_sciq.save_to_disk(\"preprocessed_sciq-qg-256-new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4e16d-63b7-4ce8-980d-d4b5a8c04efe",
   "metadata": {},
   "source": [
    "Now preprocessed has shorter context for long ones, due to map process running slow, uncomment below after getting zip file from me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5bf3694-b698-47c6-a91d-6aea4443544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "preprocessed_sciq = load_from_disk(\"preprocessed_sciq-qg-256-new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3020c7f-3b2e-4634-b55b-62dde2253b1d",
   "metadata": {},
   "source": [
    "### Step 2 Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c47179-e1f2-48f5-8e72-2de0111700d8",
   "metadata": {},
   "source": [
    "#### Tokenize for Question Generation \n",
    "\n",
    "Input : Context and Answer \\\n",
    "Output : Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cd6ea20-1618-49b8-a276-a5bc697ffefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3316e356-7e22-4385-9dba-e395bb0b890e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    text = \"{}<sep>{}\".format(example['correct_answer'], example['context'])\n",
    "    question = example['question']\n",
    "\n",
    "    max_length = 256\n",
    "    max_length_target=256\n",
    "    \n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer.encode_plus(question, max_length=max_length_target, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "    input_attention = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "    target_ids = tokenized_targets['input_ids'].squeeze()\n",
    "    target_attention = tokenized_targets['attention_mask'].squeeze()\n",
    "\n",
    "    labels = copy.deepcopy(target_ids)\n",
    "    labels[labels == 0] = -100\n",
    "    \n",
    "    outputs = {\n",
    "        'input_ids':input_ids, \n",
    "        'attention_mask': input_attention, \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "tokenized_dataset = preprocessed_sciq.map(preprocess_dataset, remove_columns= ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'context'])\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d1f5059-57ab-4fed-9b00-e7c2b089f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=2)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=2)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5278003-a089-4423-bade-02825fe217d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=3, min_delta=0.5):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18be2ea-2ad0-42d4-8298-cc6c3d1b07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1236c02-2147-449c-8443-9685befc493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:43:34.483299: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-27 20:43:34.484267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-27 20:43:34.819911: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-27 20:43:35.336587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 20:44:00.213215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def validate(model, eval_dataloader):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    model.eval()\n",
    "    progress_bar_val = tqdm(range(len(eval_dataloader)), leave=False)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        preds_flatten = torch.flatten(predictions)\n",
    "        refs_flatten = torch.flatten(batch[\"labels\"])\n",
    "        metric.add_batch(predictions=preds_flatten, references=refs_flatten)\n",
    "        progress_bar_val.update(1)\n",
    "    average_loss = total_loss/total_batches\n",
    "    val_accuracy = metric.compute()\n",
    "    progress_bar_val.close()\n",
    "    return average_loss, val_accuracy['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "576dd216-9336-437d-9d85-06fe74ef9edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs):\n",
    "    progress_bar = tqdm(range(num_training_steps), unit=\"batch\")\n",
    "    early_stopping = EarlyStopping(tolerance=2, min_delta=5)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        total_batches = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "            progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "            progress_bar.update(1)\n",
    "        train_loss = running_loss/total_batches\n",
    "        val_loss, val_acc = validate(model, eval_dataloader)\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'flan-T5-finetuned-qg-{}'.format(epoch + 1))\n",
    "        \n",
    "        print(\"Epoch {} : Training Loss {} Val Loss {} Val Acc {}%\".format(epoch + 1, train_loss, val_loss, val_acc))\n",
    "        \n",
    "        early_stopping(train_loss, val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "          print(\"We are at epoch:\", epoch)\n",
    "          break\n",
    "\n",
    "# train(model, train_dataloader, eval_dataloader, num_training_steps, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee907c19-50cb-4afa-ae4f-2e61fde22783",
   "metadata": {},
   "source": [
    "Early stopped manually at epoch 6, best model is at epoch 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "586d4beb-91ef-41b4-90c5-6ec52eab89a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3d9ca9815645f992f494da6026da8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/442 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflan-T5-finetuned-qg-3\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m evaluate_model(model, test_dataloader)\n",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_dataloader)\u001b[0m\n\u001b[1;32m     16\u001b[0m refs_flatten[refs_flatten \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m preds_questions \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(preds_flatten, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m refs_questions \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(refs_flatten, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m metric_bleu\u001b[38;5;241m.\u001b[39madd_batch(predictions\u001b[38;5;241m=\u001b[39mpreds_questions, references\u001b[38;5;241m=\u001b[39mrefs_questions)\n\u001b[1;32m     22\u001b[0m progress_bar_test\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3742\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3719\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3720\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3726\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3743\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3744\u001b[0m             seq,\n\u001b[1;32m   3745\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3746\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3747\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3748\u001b[0m         )\n\u001b[1;32m   3749\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3750\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3743\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3719\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3720\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3726\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3740\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3743\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3744\u001b[0m             seq,\n\u001b[1;32m   3745\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3746\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3747\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3748\u001b[0m         )\n\u001b[1;32m   3749\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3750\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3780\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3777\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m   3778\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m-> 3780\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m   3782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3783\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3784\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3785\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3787\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:255\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[0;32m--> 255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m framework_to_py_obj[framework](obj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mnumber):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:240\u001b[0m, in \u001b[0;36mto_py_obj.<locals>.<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_py_obj\u001b[39m(obj):\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     framework_to_py_obj \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np\u001b[38;5;241m.\u001b[39masarray(obj)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    244\u001b[0m     }\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mdict\u001b[39m, UserDict)):\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    metric_bleu = evaluate.load(\"bleu\")\n",
    "    model.eval()\n",
    "    progress_bar_test = tqdm(range(len(test_dataloader)), leave=False)\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        preds_flatten = torch.flatten(predictions)\n",
    "        refs_flatten = torch.flatten(batch[\"labels\"])\n",
    "\n",
    "        refs_flatten[refs_flatten == -100] = 0\n",
    "\n",
    "        preds_questions = tokenizer.batch_decode(preds_flatten, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        refs_questions = tokenizer.batch_decode(refs_flatten, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        metric_bleu.add_batch(predictions=preds_questions, references=refs_questions)\n",
    "        progress_bar_test.update(1)\n",
    "    test_bleu = metric_bleu.compute()\n",
    "    progress_bar_test.close()\n",
    "    return test_bleu\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\").to(device)\n",
    "model.load_state_dict(torch.load('flan-T5-finetuned-qg-3')['model_state_dict'])\n",
    "\n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c120e3be-c28e-4b05-a769-dcf99577fb15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     question \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m question[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m inference(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone-tenth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBacteria display a wide diversity of shapes and sizes. Bacterial cells are about one-tenth the size of eukaryotic cells and are typically 0.5–5.0 micrometres in length. However, a few species are visible to the unaided eye—for example, Thiomargarita namibiensis is up to half a millimetre long,[34] Epulopiscium fishelsoni reaches 0.7 mm,[35] and Thiomargarita magnifica can reach even 2 cm in length, which is 50 times larger than other known bacteria.[36][37] Among the smallest bacteria are members of the genus Mycoplasma, which measure only 0.3 micrometres, as small as the largest viruses.[38] Some bacteria may be even smaller, but these ultramicrobacteria are not well-studied.[39]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def inference(model, answer, context):\n",
    "    text = \"{}<sep>{}\".format(answer, context)\n",
    "    max_length = 256\n",
    "    tokenized_inputs = tokenizer.encode_plus(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\").to(device) \n",
    "    decoder_input_ids = tokenized_inputs['input_ids']\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=tokenized_inputs['input_ids']\n",
    "    )\n",
    "    \n",
    "    question = tokenizer.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return question[0]\n",
    "\n",
    "inference(model, \"one-tenth\", \"Bacteria display a wide diversity of shapes and sizes. Bacterial cells are about one-tenth the size of eukaryotic cells and are typically 0.5–5.0 micrometres in length. However, a few species are visible to the unaided eye—for example, Thiomargarita namibiensis is up to half a millimetre long,[34] Epulopiscium fishelsoni reaches 0.7 mm,[35] and Thiomargarita magnifica can reach even 2 cm in length, which is 50 times larger than other known bacteria.[36][37] Among the smallest bacteria are members of the genus Mycoplasma, which measure only 0.3 micrometres, as small as the largest viruses.[38] Some bacteria may be even smaller, but these ultramicrobacteria are not well-studied.[39]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8014a727-be3e-4387-854f-2ca51f36f08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b31fd9ddab74accb2e83a70b813a35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb0e319-247d-434d-a3b7-f6def2bbfe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b85b2659d84e85bf5c61ad60f96479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rizkiduwinanto/question-generation/commit/4efab07c7c0a8d4e0210dcbcd25e1f25824e51c6', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='4efab07c7c0a8d4e0210dcbcd25e1f25824e51c6', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"question-generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
