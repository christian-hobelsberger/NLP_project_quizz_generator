{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab6aa4c-053c-4e59-9278-02c80cfad56f",
   "metadata": {},
   "source": [
    "# Challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fb276a-f471-4bd6-9c85-cbb96360fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIDS\n",
      "Output of url to support generator: Question:  What is the most common way of transmitting the virus? Support:  The virus is most common in people who are infected with HIV. The virus causes AIDS..\n",
      "\n",
      "Found support in output: Support:  The virus is most common in people who are infected with HIV. The virus causes AIDS..\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' The virus is most common in people who are infected with what?',\n",
       " ' hiv',\n",
       " [' syphilis', ' gonorrhea', ' hepatitis'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import re\n",
    "import torch\n",
    "import wikipediaapi\n",
    "\n",
    "# User key for the Wikipedia API\n",
    "wiki_wiki = wikipediaapi.Wikipedia('MCQ Generation (r.j.a.lemein@student.rug.nl)', 'en')\n",
    "\n",
    "# Load the model from the saved path\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_WEB_large = BartForConditionalGeneration.from_pretrained('WEB_large/WEB_large/model_WEB_large_test').to(device)\n",
    "model_name_large = \"facebook/bart-large\"\n",
    "model_QA_large = BartForConditionalGeneration.from_pretrained(\"b-b-brouwer/CL_large\").to(device)\n",
    "tokenizer_large = BartTokenizer.from_pretrained(model_name_large)\n",
    "\n",
    "def generate_QA(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer_large(input_text, return_tensors=\"pt\").to(device) \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Generate outputs\n",
    "    outputs = model_QA_large.generate(input_ids=input_ids, max_length=1024)\n",
    "\n",
    "    # Decode the generated outputs\n",
    "    generated_text = tokenizer_large.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    question_pattern = r\"Question: (.+?\\?)\"\n",
    "    answer_pattern = r\"Answer: (.+?)(?= Distractor\\d+: |$)\"\n",
    "    dis1_pattern = r\"Distractor1: (.+)\"\n",
    "    dis2_pattern = r\"Distractor2: (.+)\"\n",
    "    dis3_pattern = r\"Distractor3: (.+)\"\n",
    "\n",
    "    question = \"\"\n",
    "    answer = \"\"\n",
    "    \n",
    "    question_match = re.search(question_pattern, generated_text)\n",
    "    answer_match = re.search(answer_pattern, generated_text)\n",
    "\n",
    "    if question_match:\n",
    "        question = question_match.group(1)\n",
    "    if answer_match:\n",
    "        answer = answer_match.group(1)\n",
    "\n",
    "    distractor_pattern = r\"Distractor\\d+: (.+?)(?= Distractor|$)\"\n",
    "    distractors = re.findall(distractor_pattern, generated_text)\n",
    "    \n",
    "    return question, answer, distractors\n",
    "\n",
    "def generate_support(url, topic, direct_wiki=False, not_print=False):\n",
    "    # Obtain input\n",
    "    if direct_wiki==False:\n",
    "        text = extract_text_from_url(url)\n",
    "        filtered_text = filter_paragraphs_by_topic(text, topic)\n",
    "    elif direct_wiki==True:\n",
    "        display_title = url.split(\"/\")[-1]\n",
    "        if not_print == False:\n",
    "            print(display_title)\n",
    "        page = wiki_wiki.page(display_title)\n",
    "        plain_text_content = page.text\n",
    "        filtered_text = filter_paragraphs_by_topic(plain_text_content, topic)\n",
    "    \n",
    "    text_label = \"Text: \"\n",
    "    answer_label = \"Answer: \"\n",
    "    \n",
    "    # Now these texts are inputs, and supports are outputs\n",
    "    merged_column_input = f\"{answer_label} {topic} {text_label} {filtered_text}\"\n",
    "\n",
    "    # Tokenize the filtered text. Note that large has the topic as an input as well!\n",
    "    inputs = tokenizer_large(merged_column_input, return_tensors=\"pt\", max_length=1024, truncation=True).to(device) \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Generate outputs\n",
    "    outputs = model_WEB_large.generate(input_ids=input_ids, max_length=1024)\n",
    "\n",
    "    # Decode the generated outputs\n",
    "    generated_text = tokenizer_large.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the generated text\n",
    "    if not_print == False:\n",
    "        print(f\"Output of url to support generator: {generated_text}.\\n\")\n",
    "    \n",
    "    # Extract the support\n",
    "    pattern = r'Support:.*$'\n",
    "    \n",
    "    # Use regular expression to find the question\n",
    "    matcher = re.search(pattern, generated_text)\n",
    "    \n",
    "    if matcher:\n",
    "        support = matcher.group(0)\n",
    "        support.strip()\n",
    "        if not_print == False:\n",
    "            print(f\"Found support in output: {support}.\\n\")  # Remove leading/trailing whitespace\n",
    "        return support\n",
    "    else:\n",
    "        if not_print == False:\n",
    "            print(\"No support found in the output string. The next model will be fed with all generated text.\\n This might cause strange results.\")\n",
    "        return generated_text\n",
    "\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Extract text from all paragraph tags\n",
    "            paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "            return ' '.join(paragraphs)  # Concatenate paragraphs into a single text\n",
    "        else:\n",
    "            print(\"Failed to fetch URL:\", url)\n",
    "            sys.stdout.flush()            \n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while fetching URL:\", str(e))\n",
    "        sys.stdout.flush()\n",
    "        return None\n",
    "\n",
    "\n",
    "def filter_paragraphs_by_topic(text, topic):\n",
    "    relevant_paragraphs = []\n",
    "    try:\n",
    "        # Split the text into paragraphs based on newline characters\n",
    "        paragraphs = text.split('\\n')\n",
    "        \n",
    "        # Simple, simply keep the paragraph if it contains the topic\n",
    "        for paragraph in paragraphs:\n",
    "            if topic.lower() in paragraph.lower():\n",
    "                relevant_paragraphs.append(paragraph)\n",
    "                \n",
    "        # However, simple doesn't always work, so\n",
    "        if len(relevant_paragraphs) == 0:\n",
    "            topics = topic.split(' ')\n",
    "            for split_topic in topics:\n",
    "                for paragraph in paragraphs:\n",
    "                    if split_topic.lower() in paragraph.lower() and paragraph not in relevant_paragraphs:\n",
    "                        relevant_paragraphs.append(paragraph)\n",
    "                    \n",
    "    except:\n",
    "        print(\"No text was fetched for this topic, as no page was found, so we cannot filter it.\")\n",
    "        sys.stdout.flush()\n",
    "        return None\n",
    "        \n",
    "                \n",
    "    return ' '.join(relevant_paragraphs)\n",
    "\n",
    "\n",
    "def generate_QA_from_url(url, topic, direct_wiki=False, not_print_support=True):\n",
    "    return generate_QA(generate_support(url, topic, direct_wiki=direct_wiki, not_print=not_print_support))\n",
    "    \n",
    "\n",
    "generate_QA_from_url(\"https://en.wikipedia.org/wiki/HIV/AIDS\", \"sex\", direct_wiki=True, not_print_support=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c67444-d827-490f-a4d5-ebf70d4aca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://312ff98db309950784.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://312ff98db309950784.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "def prompt(url, topic):\n",
    "    question, answer, distractors = generate_QA_from_url(url, topic)\n",
    "    distractors.append(answer)\n",
    "    random.shuffle(distractors)\n",
    "    return question, answer, distractors[0], distractors[1], distractors[2], distractors[3]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    answer = gr.State('')\n",
    "    d1 = gr.State('')\n",
    "    d2 = gr.State('')\n",
    "    d3 = gr.State('')\n",
    "    d4 = gr.State('')\n",
    "    \n",
    "    url = gr.Text(label=\"URL\")\n",
    "    topic = gr.Text(label=\"Topic\")\n",
    "    submit = gr.Button(\"Submit\")\n",
    "    \n",
    "    question = gr.Textbox(label=\"question\")\n",
    "    \n",
    "    choice1 = gr.Button(value=\"choice1\", interactive=True, visible=False)\n",
    "    choice2 = gr.Button(value=\"choice2\", interactive=True, visible=False)\n",
    "    choice3 = gr.Button(value=\"choice3\", interactive=True, visible=False)\n",
    "    choice4 = gr.Button(value=\"choice4\", interactive=True, visible=False)\n",
    "\n",
    "    res = gr.Text(value='correct', interactive=True, visible=False)\n",
    "\n",
    "    def check(correct_answer, answer):\n",
    "        if answer == correct_answer:\n",
    "            return gr.Text(value='correct', interactive=True, visible=True)\n",
    "        else:\n",
    "            return gr.Text(value='wrong', interactive=True, visible=True)\n",
    "\n",
    "    def on_submit(url, topic):\n",
    "        question_text, answer, choice1_text, choice2_text, choice3_text, choice4_text = prompt(url, topic)\n",
    "\n",
    "        choice1 = gr.Button(value=choice1_text, interactive=True, visible=True)\n",
    "        choice2 = gr.Button(value=choice2_text, interactive=True, visible=True)\n",
    "        choice3 = gr.Button(value=choice3_text, interactive=True, visible=True)\n",
    "        choice4 = gr.Button(value=choice4_text, interactive=True, visible=True)\n",
    "\n",
    "        return question_text, answer, choice1, choice2, choice3, choice4, choice1_text, choice2_text, choice3_text, choice4_text\n",
    "    \n",
    "    gr.on(\n",
    "        triggers=[submit.click],\n",
    "        fn=on_submit,\n",
    "        inputs=[url, topic],\n",
    "        outputs=[question, answer, choice1, choice2, choice3, choice4, d1, d2, d3, d4]\n",
    "    )\n",
    "\n",
    "    choice1.click(check, inputs=[answer, d1], outputs=res)\n",
    "    choice2.click(check, inputs=[answer, d2], outputs=res)\n",
    "    choice3.click(check, inputs=[answer, d3], outputs=res)\n",
    "    choice4.click(check, inputs=[answer, d4], outputs=res)\n",
    "    \n",
    "demo.launch(share=True)\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
